import os
import cv2
import time
import numpy as np
from threading import Thread, Lock
import queue # å¼•å…¥é˜Ÿåˆ—æ¨¡å—
import psutil
import subprocess
import re
from concurrent.futures import ThreadPoolExecutor
import multiprocessing

# âš¡ å¯ç”¨NumPyå¤šçº¿ç¨‹ä¼˜åŒ–
os.environ['OMP_NUM_THREADS'] = str(multiprocessing.cpu_count())
os.environ['OPENBLAS_NUM_THREADS'] = str(multiprocessing.cpu_count())
os.environ['MKL_NUM_THREADS'] = str(multiprocessing.cpu_count())

from ais_bench.infer.interface import InferSession
from flask import Flask, Response, render_template_string, jsonify

# --- Flask App å’Œå…¨å±€å˜é‡ (å¤šçº¿ç¨‹ä¼˜åŒ–ç‰ˆ) ---
app = Flask(__name__)
# ä½¿ç”¨æ›´å¤§çš„é˜Ÿåˆ—æ¥æ”¯æŒå¤šçº¿ç¨‹å¤„ç†
frame_queue = queue.Queue(maxsize=3)           # åŸå§‹å¸§é˜Ÿåˆ—
processed_queue = queue.Queue(maxsize=3)       # é¢„å¤„ç†åçš„æ•°æ®é˜Ÿåˆ—
result_queue = queue.Queue(maxsize=1)          # æœ€ç»ˆç»“æœé˜Ÿåˆ—

# å¤šçº¿ç¨‹é¢„å¤„ç†ç›¸å…³é˜Ÿåˆ—
preprocess_input_queue = queue.Queue(maxsize=5)
preprocess_output_queue = queue.Queue(maxsize=5)

stats_data = { # åˆå§‹åŒ–æ‰€æœ‰ç»Ÿè®¡æ•°æ® (åŒ…å«è¯¦ç»†è®¡æ—¶)
    "fps": "0.0", "pipeline_latency": "0.0", "inference_time": "0.0",
    "preprocess_time": "0.0", "postprocess_time": "0.0", "preprocess_threads": "0",
    "cpu_percent": "0.0", "mem_percent": "0.0", "npu_util": "N/A", "npu_mem": "N/A"
}
data_lock = Lock() # ä»…ç”¨äºä¿æŠ¤ stats_data çš„å†™å…¥

# --- å¯é…ç½®å¸¸é‡ (å¤šçº¿ç¨‹ä¼˜åŒ–ç‰ˆæœ¬) ---
DEVICE_ID = 0
MODEL_PATH = "./weights/fast_scnn_tusimple_bs1.om"
MODEL_WIDTH = 1024
MODEL_HEIGHT = 768
CAMERA_INDEX = 0

# ğŸ“Š æ‘„åƒå¤´åˆ†è¾¨ç‡é…ç½®
CAMERA_WIDTH = 1280   # å®æµ‹æœ€ä½³å¹³è¡¡ç‚¹
CAMERA_HEIGHT = 720

# âš¡ å¤šçº¿ç¨‹å’Œæ‰¹å¤„ç†ä¼˜åŒ–é…ç½®
PREPROCESS_THREADS = 2      # é¢„å¤„ç†çº¿ç¨‹æ•°é‡
BATCH_SIZE = 1              # æš‚æ—¶ä¿æŒ1ï¼Œé¿å…å»¶è¿Ÿå¢åŠ 
ENABLE_NUMPY_THREADING = True  # å¯ç”¨NumPyå¤šçº¿ç¨‹

NPU_SMI_PATH = "/usr/local/Ascend/driver/tools/npu-smi"

# ğŸ“Š æ‘„åƒå¤´åˆ†è¾¨ç‡é…ç½® (æ ¹æ®æ€§èƒ½éœ€æ±‚é€‰æ‹©)
# æ ¹æ®v4l2-ctlè¾“å‡ºï¼Œæ‚¨çš„æ‘„åƒå¤´æ”¯æŒä»¥ä¸‹åˆ†è¾¨ç‡@å¸§ç‡:
# 
# ğŸŸ¢ æ¨èé…ç½® (æ€§èƒ½ä¼˜å…ˆ):
# CAMERA_WIDTH = 640    # æµ‹è¯•ç»“æœ: åè€Œæ›´æ…¢ï¼Œç¼©æ”¾å¼€é”€å¤§
# CAMERA_HEIGHT = 480
#
# ğŸŸ¡ å¹³è¡¡é…ç½® (å½“å‰æœ€ä¼˜):
CAMERA_WIDTH = 1280   # å®æµ‹æœ€ä½³å¹³è¡¡ç‚¹
CAMERA_HEIGHT = 720
#
# ğŸ”´ é«˜è´¨é‡é…ç½® (è´¨é‡ä¼˜å…ˆ):
# CAMERA_WIDTH = 1920   # æœ€é«˜è´¨é‡ï¼Œé¢„å¤„ç†æ—¶é—´ ~150ms
# CAMERA_HEIGHT = 1080
#
# ğŸ’¡ ç»“è®º: 1280x720æ˜¯æœ€ä½³å¹³è¡¡ç‚¹ï¼Œç°åœ¨ä¸“æ³¨ä¼˜åŒ–å½’ä¸€åŒ–è®¡ç®—

NPU_SMI_PATH = "/usr/local/Ascend/driver/tools/npu-smi" # <--- åœ¨è¿™é‡Œå¡«å…¥ä½  'which npu-smi' æ‰¾åˆ°çš„è·¯å¾„

# --- é¢„å¤„ç†å’Œåå¤„ç†å‡½æ•° (å¤šçº¿ç¨‹ä¼˜åŒ–ç‰ˆæœ¬) ---

# é¢„è®¡ç®—å½’ä¸€åŒ–å¸¸æ•°ä»¥é¿å…é‡å¤è®¡ç®—
NORM_MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32).reshape(1, 1, 3)
NORM_STD = np.array([0.229, 0.224, 0.225], dtype=np.float32).reshape(1, 1, 3)
# é¢„è®¡ç®—ç»„åˆå¸¸æ•°: (x/255 - mean) / std = x * scale + offset
NORM_SCALE = (1.0 / 255.0) / NORM_STD
NORM_OFFSET = -NORM_MEAN / NORM_STD

def preprocess_single_optimized(img_bgr):
    """å•å¸§ä¼˜åŒ–é¢„å¤„ç†ï¼Œç”¨äºå¤šçº¿ç¨‹è°ƒç”¨"""
    # æ­¥éª¤1: å›¾åƒç¼©æ”¾
    img_resized = cv2.resize(img_bgr, (MODEL_WIDTH, MODEL_HEIGHT), interpolation=cv2.INTER_LINEAR)
    
    # æ­¥éª¤2: é¢œè‰²è½¬æ¢ + æ•°æ®ç±»å‹è½¬æ¢ï¼ˆåˆå¹¶æ“ä½œï¼‰
    img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB).astype(np.float32)
    
    # æ­¥éª¤3: è¶…çº§ä¼˜åŒ–çš„å½’ä¸€åŒ– - ä¸€æ¬¡æ€§å®Œæˆæ‰€æœ‰è®¡ç®—
    img_normalized = img_rgb * NORM_SCALE + NORM_OFFSET
    
    # æ­¥éª¤4: è½¬ç½®ï¼ˆä¸æ·»åŠ batchç»´åº¦ï¼Œåœ¨ä¸»çº¿ç¨‹ä¸­å¤„ç†ï¼‰
    img_transposed = np.transpose(img_normalized, (2, 0, 1))
    return img_transposed

def preprocess_worker():
    """é¢„å¤„ç†å·¥ä½œçº¿ç¨‹å‡½æ•°"""
    thread_id = multiprocessing.current_process().name
    print(f"é¢„å¤„ç†çº¿ç¨‹ {thread_id} å¯åŠ¨")
    
    while True:
        try:
            # è·å–å¾…å¤„ç†çš„å¸§
            item = preprocess_input_queue.get(timeout=1)
            if item is None:  # é€€å‡ºä¿¡å·
                break
                
            frame_id, frame, timestamp = item
            
            # æ‰§è¡Œé¢„å¤„ç†
            start_time = time.time()
            processed_data = preprocess_single_optimized(frame)
            process_time = (time.time() - start_time) * 1000
            
            # è¿”å›ç»“æœ
            preprocess_output_queue.put((frame_id, processed_data, process_time, timestamp))
            
        except queue.Empty:
            continue
        except Exception as e:
            print(f"é¢„å¤„ç†çº¿ç¨‹ {thread_id} é”™è¯¯: {e}")

def preprocess_batch(img_list):
    """æ‰¹å¤„ç†é¢„å¤„ç†å‡½æ•°ï¼ˆå¦‚æœéœ€è¦æ‰¹å¤„ç†ï¼‰"""
    if len(img_list) == 1:
        processed = preprocess_single_optimized(img_list[0])
        return np.ascontiguousarray(processed[np.newaxis, :, :, :], dtype=np.float32)
    
    # æ‰¹å¤„ç†é€»è¾‘ï¼ˆæš‚æ—¶ä¿ç•™å•ä¸ªå¤„ç†ï¼‰
    batch_data = []
    for img in img_list:
        processed = preprocess_single_optimized(img)
        batch_data.append(processed)
    
    return np.ascontiguousarray(np.stack(batch_data, axis=0), dtype=np.float32)

def preprocess_optimized(img_bgr):
    """é«˜åº¦ä¼˜åŒ–çš„é¢„å¤„ç†å‡½æ•°ï¼Œä¸“é—¨ä¼˜åŒ–å½’ä¸€åŒ–è®¡ç®—"""
    # æ­¥éª¤1: å›¾åƒç¼©æ”¾
    img_resized = cv2.resize(img_bgr, (MODEL_WIDTH, MODEL_HEIGHT), interpolation=cv2.INTER_LINEAR)
    
    # æ­¥éª¤2: é¢œè‰²è½¬æ¢ + æ•°æ®ç±»å‹è½¬æ¢ï¼ˆåˆå¹¶æ“ä½œï¼‰
    img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB).astype(np.float32)
    
    # æ­¥éª¤3: è¶…çº§ä¼˜åŒ–çš„å½’ä¸€åŒ– - ä¸€æ¬¡æ€§å®Œæˆæ‰€æœ‰è®¡ç®—
    # åŸæ¥: (img/255 - mean) / std
    # ä¼˜åŒ–: img * scale + offset (å…¶ä¸­ scale = 1/(255*std), offset = -mean/std)
    img_normalized = img_rgb * NORM_SCALE + NORM_OFFSET
    
    # æ­¥éª¤4: è½¬ç½®å’Œå†…å­˜æ•´ç†ï¼ˆä½¿ç”¨æ›´å¿«çš„æ–¹æ³•ï¼‰
    img_transposed = np.transpose(img_normalized, (2, 0, 1))
    input_data = np.ascontiguousarray(img_transposed[np.newaxis, :, :, :], dtype=np.float32)
    return input_data

def preprocess_original(img_bgr):
    """åŸå§‹é¢„å¤„ç†å‡½æ•°ï¼Œç”¨äºå¯¹æ¯”"""
    # ä¼˜åŒ–1: å¦‚æœè¾“å…¥åˆ†è¾¨ç‡å·²ç»æ¥è¿‘ç›®æ ‡åˆ†è¾¨ç‡ï¼Œä½¿ç”¨æ›´å¿«çš„æ’å€¼æ–¹æ³•
    img_resized = cv2.resize(img_bgr, (MODEL_WIDTH, MODEL_HEIGHT), interpolation=cv2.INTER_LINEAR)
    
    # ä¼˜åŒ–2: åˆå¹¶é¢œè‰²è½¬æ¢å’Œå½’ä¸€åŒ–æ­¥éª¤
    img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB).astype(np.float32)
    
    # ä¼˜åŒ–3: ä½¿ç”¨å‘é‡åŒ–æ“ä½œ
    img_normalized = (img_rgb / 255.0 - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])
    
    # ä¼˜åŒ–4: ç›´æ¥è½¬ç½®å¹¶ç¡®ä¿å†…å­˜è¿ç»­
    input_data = np.ascontiguousarray(img_normalized.transpose(2, 0, 1)[np.newaxis, :, :, :], dtype=np.float32)
    return input_data

# é€‰æ‹©ä½¿ç”¨å“ªä¸ªç‰ˆæœ¬ï¼ˆå¯ä»¥åˆ‡æ¢æµ‹è¯•ï¼‰
preprocess = preprocess_optimized  # ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬
# preprocess = preprocess_original  # ä½¿ç”¨åŸå§‹ç‰ˆæœ¬

def postprocess(output_tensor, original_width, original_height):
    pred_mask = np.argmax(output_tensor, axis=1).squeeze()
    vis_mask = (pred_mask * 255).astype(np.uint8)
    vis_mask_resized = cv2.resize(vis_mask, (original_width, original_height), interpolation=cv2.INTER_NEAREST)
    return vis_mask_resized

# --- æ‘„åƒå¤´æŠ“å–çº¿ç¨‹ (ä¼˜åŒ–ç‰ˆæœ¬) ---
def camera_capture_thread():
    print("æ­£åœ¨æ‰“å¼€æ‘„åƒå¤´...")
    cap = cv2.VideoCapture(CAMERA_INDEX, cv2.CAP_V4L2)
    if not cap.isOpened():
        print(f"é”™è¯¯: æ— æ³•æ‰“å¼€æ‘„åƒå¤´ç´¢å¼• {CAMERA_INDEX}.")
        return

    # è®¾ç½®MJPGæ ¼å¼ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½
    fourcc = cv2.VideoWriter_fourcc(*'MJPG')
    cap.set(cv2.CAP_PROP_FOURCC, fourcc)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, CAMERA_WIDTH)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, CAMERA_HEIGHT)
    cap.set(cv2.CAP_PROP_FPS, 30)
    
    # ä¼˜åŒ–ç¼“å†²åŒºè®¾ç½®
    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)  # å‡å°‘ç¼“å†²åŒºå»¶è¿Ÿ

    actual_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    actual_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    actual_fps = cap.get(cv2.CAP_PROP_FPS)
    print("\n--- æ‘„åƒå¤´å®é™…å‚æ•° (ä¼˜åŒ–ç‰ˆ) ---")
    print(f"åˆ†è¾¨ç‡: {actual_w}x{actual_h}, å¸§ç‡: {actual_fps}")
    print(f"ç†è®ºé¢„å¤„ç†æ•°æ®é‡å‡å°‘: {((1920*1080)/(actual_w*actual_h)):.1f}x")
    print("------------------------------------\n")

    while True:
        ret, frame = cap.read()
        if not ret:
            time.sleep(0.1)
            continue
        
        try:
            # å°è¯•æ”¾å…¥é˜Ÿåˆ—ï¼Œå¦‚æœé˜Ÿåˆ—æ»¡äº†ï¼ˆå› ä¸ºæ¨ç†çº¿ç¨‹æ…¢ï¼‰ï¼Œå°±ç›´æ¥ä¸¢å¼ƒæ—§çš„
            frame_queue.put_nowait(frame)
        except queue.Full:
            pass # é˜Ÿåˆ—å·²æ»¡ï¼Œå¿½ç•¥è¿™ä¸€å¸§ï¼Œç­‰å¾…ä¸‹ä¸€æ¬¡å¾ªç¯
    cap.release()


# --- å¤šçº¿ç¨‹æ¨ç†ç³»ç»Ÿ ---
def inference_thread_v2():
    """å¤šçº¿ç¨‹ä¼˜åŒ–çš„æ¨ç†çº¿ç¨‹"""
    global stats_data, data_lock
    print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
    model = InferSession(DEVICE_ID, MODEL_PATH)
    print("æ¨¡å‹åŠ è½½å®Œæˆã€‚")
    
    # å¯åŠ¨é¢„å¤„ç†å·¥ä½œçº¿ç¨‹æ± 
    preprocess_executor = ThreadPoolExecutor(max_workers=PREPROCESS_THREADS, thread_name_prefix="Preprocess")
    
    # æ€§èƒ½ç»Ÿè®¡å˜é‡
    frame_count = 0
    total_times = {
        "preprocess": 0,
        "inference": 0, 
        "postprocess": 0,
        "pipeline": 0
    }
    
    active_preprocess_tasks = {}  # è·Ÿè¸ªæ´»è·ƒçš„é¢„å¤„ç†ä»»åŠ¡
    frame_id_counter = 0
    
    print(f"\n=== å¼€å§‹å¤šçº¿ç¨‹æ€§èƒ½ç›‘æ§ (é¢„å¤„ç†çº¿ç¨‹æ•°: {PREPROCESS_THREADS}) ===")
    print("æ¯10å¸§è¾“å‡ºä¸€æ¬¡è¯¦ç»†æ€§èƒ½åˆ†æ...")

    while True:
        try:
            # è·å–åŸå§‹å¸§
            frame = frame_queue.get(timeout=0.1)
            pipeline_start_time = time.time()
            
            cam_height, cam_width = frame.shape[:2]
            frame_id = frame_id_counter
            frame_id_counter += 1
            
            # æäº¤é¢„å¤„ç†ä»»åŠ¡åˆ°çº¿ç¨‹æ± 
            preprocess_start_time = time.time()
            future = preprocess_executor.submit(preprocess_single_optimized, frame)
            active_preprocess_tasks[frame_id] = (future, preprocess_start_time, pipeline_start_time, cam_width, cam_height)
            
            # æ£€æŸ¥å·²å®Œæˆçš„é¢„å¤„ç†ä»»åŠ¡
            completed_tasks = []
            for fid, (fut, prep_start, pipe_start, w, h) in active_preprocess_tasks.items():
                if fut.done():
                    try:
                        processed_data = fut.result()
                        preprocess_time_ms = (time.time() - prep_start) * 1000
                        
                        # æ·»åŠ batchç»´åº¦å¹¶æ‰§è¡Œæ¨ç†
                        input_data = np.ascontiguousarray(processed_data[np.newaxis, :, :, :], dtype=np.float32)
                        
                        # æ¨ç†
                        infer_start_time = time.time()
                        outputs = model.infer([input_data])
                        inference_time_ms = (time.time() - infer_start_time) * 1000
                        
                        # åå¤„ç†
                        postprocess_start = time.time()
                        pred_mask = np.argmax(outputs[0], axis=1).squeeze()
                        vis_mask = (pred_mask * 255).astype(np.uint8)
                        lane_mask = cv2.resize(vis_mask, (w, h), interpolation=cv2.INTER_NEAREST)
                        postprocess_time_ms = (time.time() - postprocess_start) * 1000
                        
                        pipeline_latency_ms = (time.time() - pipe_start) * 1000
                        
                        # ç»Ÿè®¡
                        frame_count += 1
                        total_times["preprocess"] += preprocess_time_ms
                        total_times["inference"] += inference_time_ms
                        total_times["postprocess"] += postprocess_time_ms
                        total_times["pipeline"] += pipeline_latency_ms
                        
                        # è¾“å‡ºç»“æœ
                        try:
                            result_queue.put_nowait({
                                "frame": frame,  # ä½¿ç”¨åŸå§‹å¸§
                                "mask": lane_mask,
                                "latency": pipeline_latency_ms,
                                "inference_time": inference_time_ms,
                                "preprocess_time": preprocess_time_ms,
                                "postprocess_time": postprocess_time_ms
                            })
                        except queue.Full:
                            pass
                        
                        # æ›´æ–°ç»Ÿè®¡
                        with data_lock:
                            stats_data["pipeline_latency"] = f"{pipeline_latency_ms:.1f}"
                            stats_data["inference_time"] = f"{inference_time_ms:.1f}"
                            stats_data["preprocess_time"] = f"{preprocess_time_ms:.1f}"
                            stats_data["postprocess_time"] = f"{postprocess_time_ms:.1f}"
                            stats_data["preprocess_threads"] = str(len(active_preprocess_tasks))
                        
                        # æ¯10å¸§è¾“å‡ºæ€§èƒ½åˆ†æ
                        if frame_count % 10 == 0:
                            avg_preprocess = total_times["preprocess"] / frame_count
                            avg_inference = total_times["inference"] / frame_count
                            avg_postprocess = total_times["postprocess"] / frame_count
                            avg_pipeline = total_times["pipeline"] / frame_count
                            
                            print(f"\n--- ç¬¬{frame_count}å¸§æ€§èƒ½åˆ†æ (å¤šçº¿ç¨‹ä¼˜åŒ–ç‰ˆ) ---")
                            print(f"è¾“å…¥å¸§å°ºå¯¸: {w}x{h}")
                            print(f"æ¨¡å‹è¾“å…¥å°ºå¯¸: {MODEL_WIDTH}x{MODEL_HEIGHT}")
                            print(f"æ´»è·ƒé¢„å¤„ç†çº¿ç¨‹: {len(active_preprocess_tasks)}/{PREPROCESS_THREADS}")
                            print(f"")
                            print(f"ã€é¢„å¤„ç†ã€‘: {preprocess_time_ms:.1f}ms (å¤šçº¿ç¨‹å¹¶è¡Œ)")
                            print(f"ã€æ¨ç†æ—¶é—´ã€‘: {inference_time_ms:.1f}ms")
                            print(f"ã€åå¤„ç†ã€‘: {postprocess_time_ms:.1f}ms")
                            print(f"ã€æµæ°´çº¿æ€»å»¶è¿Ÿã€‘: {pipeline_latency_ms:.1f}ms")
                            print(f"ã€ç†è®ºæœ€å¤§FPSã€‘: {1000/pipeline_latency_ms:.1f}")
                            print(f"")
                            print(f"ã€å¹³å‡æ€§èƒ½(æœ€è¿‘{frame_count}å¸§)ã€‘")
                            print(f"  é¢„å¤„ç†: {avg_preprocess:.1f}ms")
                            print(f"  æ¨ç†: {avg_inference:.1f}ms") 
                            print(f"  åå¤„ç†: {avg_postprocess:.1f}ms")
                            print(f"  æ€»å»¶è¿Ÿ: {avg_pipeline:.1f}ms")
                            print(f"")
                            
                            # å¤šçº¿ç¨‹æ•ˆæœè¯„ä¼°
                            theoretical_single_thread = avg_preprocess * PREPROCESS_THREADS
                            if theoretical_single_thread > avg_preprocess:
                                speedup = theoretical_single_thread / avg_preprocess
                                print(f"ğŸš€ å¤šçº¿ç¨‹ä¼˜åŒ–æ•ˆæœ:")
                                print(f"   ç†è®ºåŠ é€Ÿæ¯”: {speedup:.1f}x")
                                print(f"   é¢„å¤„ç†å¹¶è¡Œæ•ˆç‡: {(speedup/PREPROCESS_THREADS)*100:.1f}%")
                            
                            # CPUåˆ©ç”¨ç‡åˆ†æ
                            cpu_util = psutil.cpu_percent()
                            print(f"ï¿½ CPUåˆ©ç”¨ç‡åˆ†æ:")
                            print(f"   å½“å‰CPUä½¿ç”¨: {cpu_util:.1f}%")
                            if cpu_util < 80:
                                print(f"   å»ºè®®: CPUè¿˜æœ‰ä½™é‡ï¼Œå¯è€ƒè™‘å¢åŠ é¢„å¤„ç†çº¿ç¨‹")
                            else:
                                print(f"   å»ºè®®: CPUä½¿ç”¨æ¥è¿‘é¥±å’Œ")
                            
                            print("=" * 60)
                        
                        completed_tasks.append(fid)
                        
                    except Exception as e:
                        print(f"å¤„ç†å¸§ {fid} æ—¶å‡ºé”™: {e}")
                        completed_tasks.append(fid)
            
            # æ¸…ç†å·²å®Œæˆçš„ä»»åŠ¡
            for fid in completed_tasks:
                del active_preprocess_tasks[fid]
                
        except queue.Empty:
            continue
        except Exception as e:
            print(f"æ¨ç†çº¿ç¨‹é”™è¯¯: {e}")
            time.sleep(0.1)


# --- NPU/CPU ç›‘æ§çº¿ç¨‹ (å¢å¼ºé”™è¯¯è¯Šæ–­) ---
def system_monitor_loop():
    global stats_data, data_lock
    npu_error_printed = False
    
    while True:
        # æ›´æ–° CPU å’Œå†…å­˜
        cpu = psutil.cpu_percent()
        mem = psutil.virtual_memory().percent
        with data_lock:
            stats_data["cpu_percent"] = f"{cpu:.1f}"
            stats_data["mem_percent"] = f"{mem:.1f}"

        # æ›´æ–° NPU - å¢å¼ºé”™è¯¯è¯Šæ–­
        try:
            result = subprocess.run([NPU_SMI_PATH, 'info'], capture_output=True, text=True, check=True, timeout=2)
            output = result.stdout
            util_match = re.search(r'NPU\s+Utilization\s*:\s*(\d+)\s*%', output)
            mem_match = re.search(r'Memory\s+Usage\s*:\s*([\d\.]+)\s*MiB\s*/\s*([\d\.]+)\s*MiB', output)
            
            npu_util = util_match.group(1) if util_match else "N/A"
            npu_mem = f"{float(mem_match.group(1)):.0f} / {float(mem_match.group(2)):.0f} MiB" if mem_match else "N/A"

            with data_lock:
                stats_data["npu_util"] = npu_util
                stats_data["npu_mem"] = npu_mem
                
            # å¦‚æœä¹‹å‰æœ‰é”™è¯¯ï¼Œç°åœ¨æ¢å¤äº†ï¼Œæ‰“å°æ¢å¤ä¿¡æ¯
            if npu_error_printed:
                print("âœ… NPUç›‘æ§å·²æ¢å¤æ­£å¸¸")
                npu_error_printed = False
                
        except subprocess.TimeoutExpired:
            if not npu_error_printed:
                print("âš ï¸  NPUç›‘æ§è¶…æ—¶ - npu-smiå‘½ä»¤å“åº”ç¼“æ…¢")
                print(f"   ä½¿ç”¨çš„NPU_SMI_PATH: {NPU_SMI_PATH}")
                print("   å»ºè®®æ£€æŸ¥NPUé©±åŠ¨çŠ¶æ€")
                npu_error_printed = True
            with data_lock:
                stats_data["npu_util"] = "Timeout"
                stats_data["npu_mem"] = "Timeout"
        except subprocess.CalledProcessError as e:
            if not npu_error_printed:
                print(f"âŒ NPUå‘½ä»¤æ‰§è¡Œå¤±è´¥:")
                print(f"   å‘½ä»¤: {NPU_SMI_PATH} info")
                print(f"   è¿”å›ç : {e.returncode}")
                print(f"   é”™è¯¯è¾“å‡º: {e.stderr}")
                print("   å¯èƒ½åŸå› :")
                print("   1. NPU_SMI_PATHè·¯å¾„ä¸æ­£ç¡®")
                print("   2. æƒé™ä¸è¶³")
                print("   3. NPUé©±åŠ¨æœªæ­£ç¡®å®‰è£…")
                npu_error_printed = True
            with data_lock:
                stats_data["npu_util"] = "Error"
                stats_data["npu_mem"] = "Error"
        except FileNotFoundError:
            if not npu_error_printed:
                print(f"âŒ æ‰¾ä¸åˆ°npu-smiå·¥å…·:")
                print(f"   é…ç½®çš„è·¯å¾„: {NPU_SMI_PATH}")
                print("   è¯·æ‰§è¡Œ 'which npu-smi' æˆ– 'find /usr -name npu-smi' æ‰¾åˆ°æ­£ç¡®è·¯å¾„")
                print("   ç„¶åæ›´æ–°ä»£ç ä¸­çš„ NPU_SMI_PATH å˜é‡")
                npu_error_printed = True
            with data_lock:
                stats_data["npu_util"] = "NotFound"
                stats_data["npu_mem"] = "NotFound"
        except Exception as e:
            if not npu_error_printed:
                print(f"âŒ è·å–NPUä¿¡æ¯æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {type(e).__name__}: {e}")
                npu_error_printed = True
            with data_lock:
                stats_data["npu_util"] = "Unknown"
                stats_data["npu_mem"] = "Unknown"
        
        time.sleep(1) # æ¯ç§’æ›´æ–°ä¸€æ¬¡ç³»ç»ŸçŠ¶æ€

# --- HTML æ¨¡æ¿ (ä¸ä¹‹å‰ç›¸åŒ) ---
HTML_TEMPLATE = """
<!DOCTYPE html>
<html>
<head>
    <title>è½¦é“çº¿æ£€æµ‹å®æ—¶æ¨ç†</title>
    <meta charset="UTF-8">
    <style>
        body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif; margin: 0; padding: 20px; background-color: #f4f7f6; color: #333; }
        .container { max-width: 1400px; margin: 0 auto; background-color: white; padding: 20px; border-radius: 10px; box-shadow: 0 4px 15px rgba(0,0,0,0.05); }
        h1 { text-align: center; color: #1a73e8; }
        .main-content { display: flex; flex-wrap: wrap; gap: 20px; }
        .video-container { flex: 3; min-width: 600px; }
        #videoStream { width: 100%; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); background-color: #eee; }
        .stats-container { flex: 1; min-width: 300px; background-color: #f8f9fa; padding: 20px; border-radius: 8px; }
        .stats-container h2 { margin-top: 0; border-bottom: 2px solid #e0e0e0; padding-bottom: 10px; color: #3c4043; }
        .stat-grid { display: grid; grid-template-columns: 1fr; gap: 15px; }
        .stat-card { background-color: #fff; padding: 15px; border-radius: 5px; border-left: 5px solid #1a73e8; box-shadow: 0 1px 3px rgba(0,0,0,0.08); display: flex; justify-content: space-between; align-items: center; }
        .stat-card.npu { border-left-color: #34a853; }
        .stat-card.cpu { border-left-color: #fbbc05; }
        .stat-label { font-size: 14px; color: #5f6368; }
        .stat-value { font-size: 18px; font-weight: 600; color: #202124; }
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸš— è½¦é“çº¿æ£€æµ‹å®æ—¶æ¨ç†ç›‘æ§ (æ€§èƒ½ä¼˜åŒ–ç‰ˆ)</h1>
        <div class="main-content">
            <div class="video-container">
                <img id="videoStream" src="{{ url_for('video_feed') }}" alt="Live Inference Stream">
            </div>
            <div class="stats-container">
                <h2>ğŸ“Š æ€§èƒ½ç›‘æ§</h2>
                <div class="stat-grid">
                    <div class="stat-card">
                        <span class="stat-label">æ˜¾ç¤ºå¸§ç‡ (FPS)</span>
                        <span id="fps" class="stat-value">--</span>
                    </div>
                    <div class="stat-card">
                        <span class="stat-label">å¤„ç†æµæ°´çº¿å»¶è¿Ÿ (ms)</span>
                        <span id="pipeline_latency" class="stat-value">--</span>
                    </div>
                    <div class="stat-card">
                        <span class="stat-label">æ¨¡å‹æ¨ç†è€—æ—¶ (ms)</span>
                        <span id="inference_time" class="stat-value">--</span>
                    </div>
                    <div class="stat-card">
                        <span class="stat-label">é¢„å¤„ç†è€—æ—¶ (ms)</span>
                        <span id="preprocess_time" class="stat-value">--</span>
                    </div>
                    <div class="stat-card">
                        <span class="stat-label">æ´»è·ƒé¢„å¤„ç†çº¿ç¨‹</span>
                        <span id="preprocess_threads" class="stat-value">--</span>
                    </div>
                    <div class="stat-card">
                        <span class="stat-label">åå¤„ç†è€—æ—¶ (ms)</span>
                        <span id="postprocess_time" class="stat-value">--</span>
                    </div>
                    <div class="stat-card npu">
                        <span class="stat-label">NPU åˆ©ç”¨ç‡ (%)</span>
                        <span id="npu_util" class="stat-value">--</span>
                    </div>
                    <div class="stat-card npu">
                        <span class="stat-label">NPU å†…å­˜å ç”¨</span>
                        <span id="npu_mem" class="stat-value">--</span>
                    </div>
                    <div class="stat-card cpu">
                        <span class="stat-label">CPU åˆ©ç”¨ç‡ (%)</span>
                        <span id="cpu_percent" class="stat-value">--</span>
                    </div>
                    <div class="stat-card cpu">
                        <span class="stat-label">ç³»ç»Ÿå†…å­˜å ç”¨ (%)</span>
                        <span id="mem_percent" class="stat-value">--</span>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <script>
        function refreshStats() {
            fetch('/stats')
            .then(response => response.json())
            .then(data => {
                document.getElementById('fps').textContent = data.fps;
                document.getElementById('pipeline_latency').textContent = data.pipeline_latency + ' ms';
                document.getElementById('inference_time').textContent = data.inference_time + ' ms';
                document.getElementById('preprocess_time').textContent = data.preprocess_time + ' ms';
                document.getElementById('preprocess_threads').textContent = data.preprocess_threads + ' / 2';
                document.getElementById('postprocess_time').textContent = data.postprocess_time + ' ms';
                document.getElementById('npu_util').textContent = data.npu_util + ' %';
                document.getElementById('npu_mem').textContent = data.npu_mem;
                document.getElementById('cpu_percent').textContent = data.cpu_percent + ' %';
                document.getElementById('mem_percent').textContent = data.mem_percent + ' %';
            })
            .catch(error => console.error('è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥:', error));
        }
        window.onload = function() {
            refreshStats();
            setInterval(refreshStats, 1000);
        };
    </script>
</body>
</html>
"""

@app.route("/")
def index():
    return render_template_string(HTML_TEMPLATE)

@app.route("/video_feed")
def video_feed():
    def generate():
        global stats_data, data_lock
        frame_count = 0
        start_time = time.time()

        while True:
            try:
                result = result_queue.get(timeout=1)
            except queue.Empty:
                continue
            
            frame = result["frame"]
            mask = result["mask"]
            
            # --- å¯è§†åŒ– ---
            green_overlay = np.zeros_like(frame, dtype=np.uint8)
            green_overlay[mask > 0] = [0, 255, 0]
            vis_frame = cv2.addWeighted(frame, 1.0, green_overlay, 0.5, 0)
            
            # è®¡ç®—å¹¶æ›´æ–°æ˜¾ç¤ºFPS
            frame_count += 1
            elapsed = time.time() - start_time
            if elapsed > 1.0:
                fps = frame_count / elapsed
                with data_lock:
                    stats_data["fps"] = f"{fps:.1f}"
                frame_count = 0
                start_time = time.time()
                
            # ç»˜åˆ¶æ–‡å­—
            with data_lock:
                fps_text = f"Display FPS: {stats_data['fps']}"
                latency_text = f"Latency: {stats_data['pipeline_latency']} ms"
            cv2.putText(vis_frame, fps_text, (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
            cv2.putText(vis_frame, latency_text, (15, 65), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
            
            # ç¼–ç å¹¶å‘é€
            (flag, encodedImage) = cv2.imencode(".jpg", vis_frame)
            if not flag:
                continue
            yield(b'--frame\r\n' b'Content-Type: image/jpeg\r\n\r\n' + 
                  bytearray(encodedImage) + b'\r\n')

    return Response(generate(), mimetype="multipart/x-mixed-replace; boundary=frame")


@app.route("/stats")
def stats():
    with data_lock:
        return jsonify(stats_data)

if __name__ == '__main__':
    # å¯åŠ¨æ—¶çš„ç³»ç»Ÿä¿¡æ¯
    print("ğŸš€ è½¦é“çº¿æ£€æµ‹å®æ—¶æ¨ç†ç³»ç»Ÿå¯åŠ¨ (å¤šçº¿ç¨‹è¶…çº§ä¼˜åŒ–ç‰ˆ)")
    print("=" * 70)
    print(f"ğŸ“· æ‘„åƒå¤´é…ç½®:")
    print(f"   - ç›®æ ‡åˆ†è¾¨ç‡: {CAMERA_WIDTH}x{CAMERA_HEIGHT}")
    print(f"   - æ‘„åƒå¤´ç´¢å¼•: {CAMERA_INDEX}")
    print(f"ğŸ§  æ¨¡å‹é…ç½®:")
    print(f"   - æ¨¡å‹è·¯å¾„: {MODEL_PATH}")
    print(f"   - è¾“å…¥å°ºå¯¸: {MODEL_WIDTH}x{MODEL_HEIGHT}")
    print(f"   - è®¾å¤‡ID: {DEVICE_ID}")
    print(f"âš¡ å¤šçº¿ç¨‹ä¼˜åŒ–ç‰¹æ€§:")
    print(f"   - NumPyå¤šçº¿ç¨‹: å¯ç”¨ (CPUæ ¸å¿ƒ: {multiprocessing.cpu_count()})")
    print(f"   - é¢„å¤„ç†çº¿ç¨‹æ•°: {PREPROCESS_THREADS}")
    print(f"   - æ‰¹å¤„ç†å¤§å°: {BATCH_SIZE}")
    print(f"   - é¢„è®¡ç®—å½’ä¸€åŒ–å¸¸æ•°")
    print(f"   - å¼‚æ­¥é¢„å¤„ç†æµæ°´çº¿")
    print(f"ğŸ’» ç³»ç»Ÿä¿¡æ¯:")
    print(f"   - CPUæ ¸å¿ƒ: {psutil.cpu_count()}")
    print(f"   - ç‰©ç†å†…å­˜: {psutil.virtual_memory().total / (1024**3):.1f} GB")
    print(f"   - NPUå·¥å…·è·¯å¾„: {NPU_SMI_PATH}")
    print("=" * 70)
    
    # å¯åŠ¨æ‘„åƒå¤´æŠ“å–çº¿ç¨‹
    cam_thread = Thread(target=camera_capture_thread, daemon=True)
    cam_thread.start()
    
    # å¯åŠ¨å¤šçº¿ç¨‹æ¨ç†çº¿ç¨‹
    inf_thread = Thread(target=inference_thread_v2, daemon=True)
    inf_thread.start()

    # å¯åŠ¨ç³»ç»Ÿç›‘æ§çº¿ç¨‹
    monitor_thread = Thread(target=system_monitor_loop, daemon=True)
    monitor_thread.start()
    
    print("WebæœåŠ¡å™¨å·²å¯åŠ¨ã€‚è¯·åœ¨æµè§ˆå™¨ä¸­è®¿é—®: http://<Your_Atlas_IP>:8000")
    print("ç»ˆç«¯å°†æ˜¾ç¤ºè¯¦ç»†çš„å¤šçº¿ç¨‹æ€§èƒ½åˆ†æä¿¡æ¯...")
    app.run(host='0.0.0.0', port=8000, threaded=True)