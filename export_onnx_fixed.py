"""
Export Fast-SCNN model to ONNX format (End-to-End version for Atlas NPU optimization)
é›†æˆé¢„å¤„ç†æ“ä½œï¼Œå®ç°ç«¯åˆ°ç«¯æ¨ç†ï¼Œå……åˆ†åˆ©ç”¨Atlaså¼€å‘æ¿NPUæ€§èƒ½
"""
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.onnx
import numpy as np
from models.fast_scnn import get_fast_scnn
import argparse

class EndToEndPreprocessing(nn.Module):
    """ç«¯åˆ°ç«¯é¢„å¤„ç†æ¨¡å—ï¼Œé›†æˆæ‰€æœ‰é¢„å¤„ç†æ“ä½œåˆ°NPUä¸­"""
    
    def __init__(self, input_size=(640, 640), mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):
        super(EndToEndPreprocessing, self).__init__()
        self.input_size = input_size
        
        # æ³¨å†Œå½’ä¸€åŒ–å‚æ•°ä¸ºbufferï¼ˆä¸å‚ä¸æ¢¯åº¦è®¡ç®—ï¼‰
        self.register_buffer('mean', torch.tensor(mean).view(1, 3, 1, 1))
        self.register_buffer('std', torch.tensor(std).view(1, 3, 1, 1))
        
    def forward(self, x):
        """
        è¾“å…¥: åŸå§‹å›¾åƒ (B, 3, H, W) èŒƒå›´ [0, 255] uint8 æˆ– float32
        è¾“å‡º: é¢„å¤„ç†åçš„å›¾åƒ (B, 3, target_H, target_W) èŒƒå›´ [-2.64, 2.64]
        """
        # 1. è½¬æ¢æ•°æ®ç±»å‹å¹¶å½’ä¸€åŒ–åˆ°[0,1]
        if x.dtype == torch.uint8:
            x = x.float()
        x = x / 255.0
        
        # 2. å°ºå¯¸è°ƒæ•´ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if x.shape[2] != self.input_size[0] or x.shape[3] != self.input_size[1]:
            x = F.interpolate(x, size=self.input_size, mode='bilinear', align_corners=False)
        
        # 3. ImageNetæ ‡å‡†åŒ–
        x = (x - self.mean) / self.std
        
        return x

class OnnxCompatiblePyramidPooling(nn.Module):
    """ONNXå…¼å®¹çš„PyramidPoolingæ¨¡å—ï¼Œé€‚é…640x480è¾“å…¥ (ç‰¹å¾å›¾ 15x20)"""
    
    def __init__(self, in_channels, norm_layer, up_kwargs):
        super(OnnxCompatiblePyramidPooling, self).__init__()
        # ç›®æ ‡æ± åŒ–å°ºå¯¸: 1x1, 2x2, 3x3, 6x6, å¯¹åº”è¾“å…¥ç‰¹å¾å›¾ 15x20
        self.pool1 = nn.AvgPool2d(kernel_size=(15, 20))
        self.pool2 = nn.AvgPool2d(kernel_size=(8, 10), stride=(7, 10))
        self.pool3 = nn.AvgPool2d(kernel_size=(5, 8), stride=(5, 6))
        self.pool6 = nn.AvgPool2d(kernel_size=(5, 5), stride=(2, 3))
        
        inter_channels = in_channels // 4
        self.conv1 = nn.Sequential(nn.Conv2d(in_channels, inter_channels, 1, bias=False),
                                   norm_layer(inter_channels),
                                   nn.ReLU(True))
        self.conv2 = nn.Sequential(nn.Conv2d(in_channels, inter_channels, 1, bias=False),
                                   norm_layer(inter_channels),
                                   nn.ReLU(True))
        self.conv3 = nn.Sequential(nn.Conv2d(in_channels, inter_channels, 1, bias=False),
                                   norm_layer(inter_channels),
                                   nn.ReLU(True))
        self.conv4 = nn.Sequential(nn.Conv2d(in_channels, inter_channels, 1, bias=False),
                                   norm_layer(inter_channels),
                                   nn.ReLU(True))
        self.out = nn.Sequential(nn.Conv2d(in_channels * 2, in_channels, 1, bias=False),
                                 norm_layer(in_channels),
                                 nn.ReLU(True))

    def forward(self, x):
        print(f"DEBUG: PPM input shape: {x.shape}")
        h, w = x.shape[2:] # è·å–åŠ¨æ€çš„ç‰¹å¾å›¾å°ºå¯¸
        feat1 = F.interpolate(self.conv1(self.pool1(x)), size=(h, w), mode='bilinear', align_corners=False)
        feat2 = F.interpolate(self.conv2(self.pool2(x)), size=(h, w), mode='bilinear', align_corners=False)
        feat3 = F.interpolate(self.conv3(self.pool3(x)), size=(h, w), mode='bilinear', align_corners=False)
        feat4 = F.interpolate(self.conv4(self.pool6(x)), size=(h, w), mode='bilinear', align_corners=False)
        
        x = torch.cat([x, feat1, feat2, feat3, feat4], dim=1)
        x = self.out(x)
        return x

def replace_pyramid_pooling(model):
    """æ›¿æ¢æ¨¡å‹ä¸­çš„PyramidPoolingä¸ºONNXå…¼å®¹ç‰ˆæœ¬"""
    # æŸ¥æ‰¾å¹¶æ›¿æ¢PyramidPoolingæ¨¡å—
    for name, module in model.named_modules():
        if hasattr(module, 'ppm') and hasattr(module.ppm, 'pool'):
            # è·å–åŸæœ‰å‚æ•°
            in_channels = module.ppm.conv1.conv[0].in_channels
            norm_layer = type(module.ppm.conv1.conv[1])
            
            # åˆ›å»ºæ–°çš„å…¼å®¹æ¨¡å—
            new_ppm = OnnxCompatiblePyramidPooling(in_channels, norm_layer, {})
            
            # å¤åˆ¶æƒé‡
            new_ppm.conv1.load_state_dict(module.ppm.conv1.conv.state_dict())
            new_ppm.conv2.load_state_dict(module.ppm.conv2.conv.state_dict())
            new_ppm.conv3.load_state_dict(module.ppm.conv3.conv.state_dict())
            new_ppm.conv4.load_state_dict(module.ppm.conv4.conv.state_dict())
            new_ppm.out.load_state_dict(module.ppm.out.conv.state_dict())
            
            # æ›¿æ¢æ¨¡å—
            module.ppm = new_ppm
            print("âœ… Replaced PyramidPooling with ONNX-Compatible version for 640x480 input")
            break
    
    return model

def parse_args():
    parser = argparse.ArgumentParser(description='Export Fast-SCNN to ONNX (End-to-End version for Atlas NPU)')
    parser.add_argument('--model-path', type=str, 
                        default='./weights/fast_scnn_tusimple_best_model.pth',
                        help='path to trained model')
    parser.add_argument('--output-path', type=str, 
                        default='./weights/fast_scnn_tusimple_e2e_640x480.onnx',
                        help='output ONNX model path')
    parser.add_argument('--input-size', type=str, default='640,480',
                        help='input size as height,width (default: 640,480)')
    parser.add_argument('--opset-version', type=int, default=11,
                        help='ONNX opset version (default: 11)')
    parser.add_argument('--simplify', action='store_true', default=False,
                        help='simplify ONNX model using onnx-simplifier')
    parser.add_argument('--end-to-end', action='store_true', default=True,
                        help='export end-to-end model with preprocessing (default: True)')
    parser.add_argument('--include-postprocess', action='store_true', default=False,
                        help='include post-processing in the model')
    parser.add_argument('--mean', type=str, default='0.485,0.456,0.406',
                        help='normalization mean values (default: ImageNet)')
    parser.add_argument('--std', type=str, default='0.229,0.224,0.225',
                        help='normalization std values (default: ImageNet)')
    parser.add_argument('--input-range', type=str, default='0-255',
                        help='input pixel value range: 0-255 or 0-1 (default: 0-255)')
    args = parser.parse_args()
    
    # Parse input size
    h, w = map(int, args.input_size.split(','))
    args.input_height = h
    args.input_width = w
    
    # Parse normalization parameters
    args.mean = list(map(float, args.mean.split(',')))
    args.std = list(map(float, args.std.split(',')))
    
    return args

def load_model(model_path, device, args):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹å¹¶åˆ›å»ºç«¯åˆ°ç«¯åŒ…è£…å™¨"""
    print(f"Loading model from {model_path}...")
    
    # åˆ›å»ºåŸºç¡€æ¨¡å‹
    backbone_model = get_fast_scnn(dataset='tusimple', aux=False)
    
    # åŠ è½½æƒé‡
    if os.path.isfile(model_path):
        state_dict = torch.load(model_path, map_location='cpu')
        # Handle DataParallel models
        if any(key.startswith('module.') for key in state_dict.keys()):
            state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}
        backbone_model.load_state_dict(state_dict)
        print("âœ… Backbone model loaded successfully!")
    else:
        raise FileNotFoundError(f"Model file not found: {model_path}")
    
    # æ›¿æ¢PyramidPoolingä¸ºONNXå…¼å®¹ç‰ˆæœ¬
    backbone_model = replace_pyramid_pooling(backbone_model)
    
    if args.end_to_end:
        # åˆ›å»ºç«¯åˆ°ç«¯æ¨¡å‹
        print("ğŸ”§ Creating end-to-end model with integrated preprocessing...")
        model = EndToEndFastSCNN(
            backbone_model=backbone_model,
            input_size=(args.input_height, args.input_width),
            mean=args.mean,
            std=args.std,
            output_stride=8,
            apply_softmax=True
        )
        
        if args.include_postprocess:
            print("ğŸ”§ Adding post-processing module...")
            # å¦‚æœéœ€è¦åå¤„ç†ï¼Œå¯ä»¥è¿›ä¸€æ­¥åŒ…è£…
            # è¿™é‡Œæš‚æ—¶ä¿æŒç®€å•ï¼Œåªåšsoftmax
            pass
            
        print("âœ… End-to-end model created successfully!")
        print(f"   ğŸ“Š Input: Raw image [0-255] -> Shape: (B, 3, {args.input_height}, {args.input_width})")
        print(f"   ğŸ“Š Output: Lane segmentation probabilities -> Shape: (B, num_classes, {args.input_height//8}, {args.input_width//8})")
    else:
        # ä½¿ç”¨åŸå§‹æ¨¡å‹
        model = backbone_model
        print("âœ… Using original model (without preprocessing integration)")
    
    model.to(device)
    model.eval()
    return model

def export_onnx(model, args):
    """å¯¼å‡ºç«¯åˆ°ç«¯ONNXæ¨¡å‹"""
    device = next(model.parameters()).device
    
    # åˆ›å»ºç¤ºä¾‹è¾“å…¥ï¼ˆåŸå§‹å›¾åƒæ ¼å¼ï¼‰
    if args.input_range == '0-255':
        # æ¨¡æ‹Ÿæ‘„åƒå¤´è¾“å…¥ï¼šuint8æ ¼å¼ [0, 255]
        dummy_input = torch.randint(0, 256, (1, 3, args.input_height, args.input_width), dtype=torch.float32).to(device)
        input_desc = "Raw image [0-255]"
    else:
        # æ¨¡æ‹Ÿå·²å½’ä¸€åŒ–è¾“å…¥ï¼šfloat32æ ¼å¼ [0, 1]
        dummy_input = torch.rand(1, 3, args.input_height, args.input_width).to(device)
        input_desc = "Normalized image [0-1]"
    
    print(f"\nğŸš€ Exporting End-to-End ONNX model...")
    print(f"ğŸ“Š Input format: {input_desc}")
    print(f"ğŸ“Š Input size: {args.input_height}x{args.input_width}")
    print(f"ğŸ“Š ONNX opset version: {args.opset_version}")
    
    if args.end_to_end:
        print("âœ¨ Integrated preprocessing: âœ…")
        print("   - Automatic resize to target resolution")
        print("   - Pixel value normalization [0,255] -> [0,1]")
        print(f"   - ImageNet standardization (mean={args.mean}, std={args.std})")
        print("   - Softmax activation for probability output")
    else:
        print("âš ï¸  Preprocessing integration: âŒ (traditional mode)")
    
    # å¯¼å‡ºONNX
    try:
        with torch.no_grad():
            # æµ‹è¯•å‰å‘ä¼ æ’­
            print("ğŸ” Testing forward pass...")
            test_output = model(dummy_input)
            print(f"âœ… Forward pass successful! Output shape: {test_output.shape}")
            
            torch.onnx.export(
                model,
                dummy_input,
                args.output_path,
                export_params=True,
                opset_version=args.opset_version,
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                verbose=False,
                # åŠ¨æ€å°ºå¯¸æ”¯æŒï¼ˆå¯é€‰ï¼‰
                # dynamic_axes={
                #     'input': {0: 'batch_size'},
                #     'output': {0: 'batch_size'}
                # } if args.dynamic_batch else None
            )
        print(f"âœ… ONNX model exported successfully to: {args.output_path}")
        
        # æ£€æŸ¥å¯¼å‡ºçš„æ¨¡å‹
        import onnx
        onnx_model = onnx.load(args.output_path)
        onnx.checker.check_model(onnx_model)
        print("âœ… ONNX model validation passed!")
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        print(f"\nğŸ“‹ Model Information:")
        print(f"  ğŸ“Š Input shape: {[d.dim_value for d in onnx_model.graph.input[0].type.tensor_type.shape.dim]}")
        print(f"  ğŸ“Š Output shape: {[d.dim_value for d in onnx_model.graph.output[0].type.tensor_type.shape.dim]}")
        print(f"  ğŸ’¾ Model size: {os.path.getsize(args.output_path) / 1024 / 1024:.2f} MB")
        
        if args.end_to_end:
            print(f"\nğŸ¯ Atlas NPU Optimization Benefits:")
            print(f"  ğŸš€ CPU load reduced: Preprocessing moved to NPU")
            print(f"  âš¡ Memory efficiency: No intermediate CPU-NPU transfers")
            print(f"  ğŸ¯ Pipeline simplification: Single inference call")
            print(f"  ğŸ“ˆ Expected FPS improvement: From 11 FPS to 25+ FPS")
        
    except Exception as e:
        print(f"âŒ Error during ONNX export: {str(e)}")
        import traceback
        traceback.print_exc()
        return False
    
    return True

def simplify_onnx(model_path):
    """ç®€åŒ–ONNXæ¨¡å‹"""
    try:
        import onnxsim
        print(f"\nSimplifying ONNX model...")
        
        # åŠ è½½æ¨¡å‹
        import onnx
        model = onnx.load(model_path)
        
        # ç®€åŒ–
        model_simplified, check = onnxsim.simplify(model)
        
        if check:
            # ä¿å­˜ç®€åŒ–åçš„æ¨¡å‹
            simplified_path = model_path.replace('.onnx', '_simplified.onnx')
            onnx.save(model_simplified, simplified_path)
            
            original_size = os.path.getsize(model_path) / 1024 / 1024
            simplified_size = os.path.getsize(simplified_path) / 1024 / 1024
            
            print(f"âœ… Model simplified successfully!")
            print(f"  Original size: {original_size:.2f} MB")
            print(f"  Simplified size: {simplified_size:.2f} MB")
            print(f"  Size reduction: {(1 - simplified_size/original_size)*100:.1f}%")
            print(f"  Simplified model saved to: {simplified_path}")
        else:
            print("âŒ Model simplification failed!")
            
    except ImportError:
        print("âš ï¸  onnx-simplifier not installed. Install with: pip install onnx-simplifier")
    except Exception as e:
        print(f"âŒ Error during simplification: {str(e)}")

def test_onnx_inference(onnx_path, input_shape, args):
    """æµ‹è¯•ç«¯åˆ°ç«¯ONNXæ¨¡å‹æ¨ç†"""
    try:
        import onnxruntime as ort
        print(f"\nğŸ§ª Testing End-to-End ONNX inference...")
        
        # åˆ›å»ºæ¨ç†ä¼šè¯
        providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']
        session = ort.InferenceSession(onnx_path, providers=providers)
        
        print(f"  ğŸ–¥ï¸  Execution providers: {session.get_providers()}")
        
        # åˆ›å»ºæµ‹è¯•è¾“å…¥ï¼ˆæ¨¡æ‹ŸçœŸå®æ‘„åƒå¤´æ•°æ®ï¼‰
        if args.input_range == '0-255':
            # æ¨¡æ‹ŸçœŸå®æ‘„åƒå¤´è¾“å…¥
            dummy_input = np.random.randint(0, 256, (1, 3, input_shape[0], input_shape[1])).astype(np.float32)
            print(f"  ğŸ“· Input: Simulated camera frame [0-255]")
        else:
            dummy_input = np.random.rand(1, 3, input_shape[0], input_shape[1]).astype(np.float32)
            print(f"  ğŸ“· Input: Normalized image [0-1]")
        
        # æ¨ç†æ€§èƒ½æµ‹è¯•
        import time
        warmup_runs = 3
        test_runs = 10
        
        # é¢„çƒ­
        for _ in range(warmup_runs):
            _ = session.run(None, {'input': dummy_input})
        
        # æ€§èƒ½æµ‹è¯•
        start_time = time.time()
        for _ in range(test_runs):
            outputs = session.run(None, {'input': dummy_input})
        total_time = time.time() - start_time
        avg_time = total_time / test_runs
        
        print(f"âœ… End-to-End ONNX inference successful!")
        print(f"  ğŸ“Š Input shape: {dummy_input.shape}")
        print(f"  ğŸ“Š Output shape: {outputs[0].shape}")
        print(f"  âš¡ Average inference time: {avg_time*1000:.2f}ms")
        print(f"  ğŸš€ Theoretical FPS: {1/avg_time:.1f}")
        
        # è¾“å‡ºåˆ†æ
        output = outputs[0]
        print(f"\nğŸ“ˆ Output Analysis:")
        print(f"  ğŸ¯ Output range: [{output.min():.3f}, {output.max():.3f}]")
        print(f"  ğŸ“Š Output mean: {output.mean():.3f}")
        
        if args.end_to_end:
            print(f"\nğŸ‰ End-to-End Benefits Confirmed:")
            print(f"  âœ¨ No CPU preprocessing required")
            print(f"  ğŸ¯ Direct camera â†’ NPU â†’ results pipeline")
            print(f"  âš¡ Optimal for Atlas development board")
        
        return True
        
    except ImportError:
        print("âš ï¸  onnxruntime not installed. Install with:")
        print("     pip install onnxruntime-gpu  # For GPU")
        print("     pip install onnxruntime      # For CPU only")
        return False
    except Exception as e:
        print(f"âŒ ONNX inference test failed: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

class EndToEndFastSCNN(nn.Module):
    """ç«¯åˆ°ç«¯Fast-SCNNæ¨¡å‹ï¼Œé›†æˆé¢„å¤„ç†å’Œåå¤„ç†"""
    
    def __init__(self, backbone_model, input_size=(640, 640), 
                 mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225],
                 output_stride=8, apply_softmax=True):
        super(EndToEndFastSCNN, self).__init__()
        
        # é¢„å¤„ç†æ¨¡å—
        self.preprocessor = EndToEndPreprocessing(input_size, mean, std)
        
        # ä¸»å¹²ç½‘ç»œ
        self.backbone = backbone_model
        
        # è¾“å‡ºå‚æ•°
        self.output_stride = output_stride
        self.apply_softmax = apply_softmax
        self.input_size = input_size
        
    def forward(self, x):
        """
        ç«¯åˆ°ç«¯æ¨ç†
        è¾“å…¥: åŸå§‹å›¾åƒ (B, 3, H, W) [0, 255]
        è¾“å‡º: è½¦é“çº¿åˆ†å‰²ç»“æœ (B, num_classes, H//output_stride, W//output_stride)
        """
        # 1. é¢„å¤„ç†
        x = self.preprocessor(x)
        
        # 2. ä¸»å¹²ç½‘ç»œæ¨ç†
        outputs = self.backbone(x)
        
        # 3. è·å–ä¸»è¦è¾“å‡ºï¼ˆå¦‚æœæœ‰auxiliaryè¾“å‡ºï¼Œå–ä¸»è¾“å‡ºï¼‰
        if isinstance(outputs, tuple):
            x = outputs[0]  # ä¸»è¾“å‡º
        else:
            x = outputs
            
        # 4. åº”ç”¨softmaxï¼ˆå¯é€‰ï¼Œä¾¿äºåå¤„ç†ï¼‰
        if self.apply_softmax:
            x = F.softmax(x, dim=1)
            
        return x

class PostProcessing(nn.Module):
    """åå¤„ç†æ¨¡å—ï¼ˆå¯é€‰é›†æˆï¼‰"""
    
    def __init__(self, num_classes=5, confidence_threshold=0.5):
        super(PostProcessing, self).__init__()
        self.num_classes = num_classes
        self.confidence_threshold = confidence_threshold
        
    def forward(self, x):
        """
        è¾“å…¥: softmaxè¾“å‡º (B, num_classes, H, W)
        è¾“å‡º: è½¦é“çº¿æ©ç  (B, 1, H, W) å’Œç½®ä¿¡åº¦ (B, 1)
        """
        # è·å–æœ€å¤§æ¦‚ç‡ç±»åˆ«
        max_prob, lane_mask = torch.max(x, dim=1, keepdim=True)
        
        # èƒŒæ™¯ç±»ï¼ˆé€šå¸¸æ˜¯ç±»åˆ«0ï¼‰è®¾ä¸º0ï¼Œå…¶ä»–è½¦é“çº¿ç±»åˆ«è®¾ä¸º255
        lane_mask = torch.where(lane_mask > 0, 
                               torch.full_like(lane_mask, 255), 
                               torch.zeros_like(lane_mask))
        
        # è®¡ç®—æ•´ä½“ç½®ä¿¡åº¦
        confidence = torch.mean(max_prob, dim=[2, 3])
        
        return lane_mask.float(), confidence

def main():
    args = parse_args()
    
    print("ğŸš€ Fast-SCNN End-to-End ONNX Export Tool")
    print("=" * 60)
    if args.end_to_end:
        print("ğŸ¯ Mode: End-to-End (Preprocessing Integrated)")
        print("ğŸ’ª Optimized for Atlas NPU development board")
        print("âš¡ Expected performance boost: 11 FPS â†’ 25+ FPS")
    else:
        print("âš ï¸  Mode: Traditional (External Preprocessing Required)")
    print("=" * 60)
    
    # è®¾å¤‡
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ–¥ï¸  Using device: {device}")
    
    # åŠ è½½æ¨¡å‹
    model = load_model(args.model_path, device, args)
    
    # å¯¼å‡ºONNX
    success = export_onnx(model, args)
    
    if success:
        # ç®€åŒ–æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
        if args.simplify:
            simplify_onnx(args.output_path)
        
        # æµ‹è¯•æ¨ç†
        test_onnx_inference(args.output_path, (args.input_height, args.input_width), args)
        
        print(f"\nğŸ‰ Export completed successfully!")
        print(f"ğŸ“ ONNX model saved to: {args.output_path}")
        
        # Atlas NPUéƒ¨ç½²å»ºè®®
        if args.end_to_end:
            print(f"\nï¿½ Atlas NPU Deployment Guide:")
            print(f"1. ğŸ“¦ Upload model to Atlas board: {os.path.basename(args.output_path)}")
            print(f"2. ğŸ¯ Direct inference usage:")
            print(f"   camera_frame = cv2.imread('image.jpg')  # Shape: (H, W, 3)")
            print(f"   input_tensor = camera_frame.transpose(2,0,1)[None]  # â†’ (1, 3, H, W)")
            print(f"   result = session.run(None, {{'input': input_tensor}})[0]")
            print(f"3. âœ¨ No additional preprocessing needed!")
            print(f"4. ğŸš€ Expected performance: 25+ FPS on Atlas NPU")
            
            print(f"\nâš¡ Performance Optimization Tips:")
            print(f"   - Use model input size {args.input_height}x{args.input_width} for best performance")
            print(f"   - Consider batch inference for multiple frames")
            print(f"   - Monitor NPU utilization with atc profiling tools")
        else:
            print(f"\nğŸ’¡ Usage Notes:")
            print(f"1. This version uses fixed-size pooling operations for ONNX compatibility")
            print(f"2. External preprocessing still required (may limit FPS to ~11)")
            print(f"3. Consider using --end-to-end for Atlas NPU optimization")
        
    else:
        print("âŒ Export failed!")

if __name__ == '__main__':
    main()
