import os
import cv2
import time
import numpy as np
from threading import Thread, Lock
import queue # å¼•å…¥é˜Ÿåˆ—æ¨¡å—
import psutil
import subprocess
import re

from ais_bench.infer.interface import InferSession
from flask import Flask, Response, render_template_string, jsonify

# --- Flask App å’Œå…¨å±€å˜é‡ ---
app = Flask(__name__)
frame_queue = queue.Queue(maxsize=1)
result_queue = queue.Queue(maxsize=1)
stats_data = {
    "fps": "0.0", "pipeline_latency": "0.0", "inference_time": "0.0",
    "preprocess_time": "0.0", "postprocess_time": "0.0",
    "cpu_percent": "0.0", "mem_percent": "0.0", "npu_util": "N/A", "npu_mem": "N/A"
}
data_lock = Lock()

# --- å¯é…ç½®å¸¸é‡ ---
DEVICE_ID = 0
MODEL_PATH = "./weights/fast_scnn_tusimple_bs1.om"
MODEL_WIDTH = 640
MODEL_HEIGHT = 640
CAMERA_INDEX = 0
CAMERA_WIDTH = 640
CAMERA_HEIGHT = 640
NPU_SMI_PATH = "/usr/local/Ascend/driver/tools/npu-smi"

# --------------------------------------------------------------------------
# --- âš¡âš¡âš¡ æ–°å¢å’Œä¿®æ”¹çš„æ ¸å¿ƒéƒ¨åˆ†ï¼šé¢„å¤„ç†å‡½æ•° âš¡âš¡âš¡ ---
# --------------------------------------------------------------------------

# é¢„è®¡ç®—å¸¸æ•°ï¼ˆä¿ç•™ç»™åŸå§‹ä¼˜åŒ–ç‰ˆå¯¹æ¯”ç”¨ï¼‰
NORM_MEAN_NP = np.array([0.485, 0.456, 0.406], dtype=np.float32).reshape(1, 1, 3)
NORM_STD_NP = np.array([0.229, 0.224, 0.225], dtype=np.float32).reshape(1, 1, 3)
NORM_SCALE = (1.0 / 255.0) / NORM_STD_NP
NORM_OFFSET = -NORM_MEAN_NP / NORM_STD_NP

def preprocess_optimized_numpy(img_bgr, dtype=np.float32):
    """ä½ åŸæ¥çš„ä¼˜åŒ–ç‰ˆï¼ŒåŸºäºNumPyï¼Œç”¨äºå¯¹æ¯”ã€‚å¢åŠ äº†dtypeå‚æ•°ã€‚"""
    img_resized = cv2.resize(img_bgr, (MODEL_WIDTH, MODEL_HEIGHT), interpolation=cv2.INTER_LINEAR)
    img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB).astype(np.float32)
    img_normalized = img_rgb * NORM_SCALE + NORM_OFFSET
    img_transposed = np.transpose(img_normalized, (2, 0, 1))
    input_data = np.ascontiguousarray(img_transposed[np.newaxis, :, :, :], dtype=dtype)
    return input_data

# --- è¿™æ˜¯æˆ‘ä»¬çš„æ–°ç‹ç‰Œï¼šç»“åˆ cv2.dnn.blobFromImage å’Œ FP16 ---
# ä¸º blobFromImage å‡†å¤‡çš„å¸¸æ•°
BLOB_MEAN_FOR_SUBTRACTION = np.array([0.485, 0.456, 0.406], dtype=np.float32).reshape(1, 3, 1, 1)
BLOB_STD_FOR_DIVISION = np.array([0.229, 0.224, 0.225], dtype=np.float32).reshape(1, 3, 1, 1)

def preprocess_ultimate_optimized(img_bgr, dtype=np.float16):
    """
    ç»ˆæä¼˜åŒ–é¢„å¤„ç†å‡½æ•°:
    1. ä½¿ç”¨ cv2.dnn.blobFromImage è¿›è¡Œé«˜é€Ÿ ç¼©æ”¾, BGR->RGB, /255.0ã€‚
    2. ä½¿ç”¨ NumPy å¹¿æ’­æ“ä½œå¿«é€Ÿå®Œæˆå‡å‡å€¼å’Œé™¤ä»¥æ ‡å‡†å·®ã€‚
    3. æœ€ç»ˆè½¬æ¢ä¸ºæŒ‡å®šæ•°æ®ç±»å‹ (é»˜è®¤ä¸ºFP16)ã€‚
    """
    # æ­¥éª¤1: ç”¨blobFromImageé«˜æ•ˆå®Œæˆ ç¼©æ”¾, BGR->RGBè½¬æ¢, å¹¶å½’ä¸€åŒ–åˆ°[0,1]
    # scalefactor=1.0/255.0: å°†åƒç´ å€¼ä»0-255ç¼©æ”¾åˆ°0-1
    # swapRB=True: å°†BGRå›¾åƒè½¬æ¢ä¸ºRGBï¼Œå› ä¸ºæ¨¡å‹é€šå¸¸éœ€è¦RGB
    blob = cv2.dnn.blobFromImage(img_bgr, 
                                 scalefactor=1.0/255.0, 
                                 size=(MODEL_WIDTH, MODEL_HEIGHT),
                                 swapRB=True, 
                                 crop=False) # blobçš„shapeæ˜¯(1, 3, H, W)

    # æ­¥éª¤2: æ‰‹åŠ¨è¿›è¡Œå‡å‡å€¼å’Œé™¤ä»¥æ ‡å‡†å·®
    # (blob - mean) / std
    blob -= BLOB_MEAN_FOR_SUBTRACTION
    blob /= BLOB_STD_FOR_DIVISION
    
    # æ­¥éª¤3: è½¬æ¢ä¸ºç›®æ ‡æ•°æ®ç±»å‹å¹¶ç¡®ä¿å†…å­˜è¿ç»­æ€§
    # å¦‚æœä½ çš„æ¨¡å‹éœ€è¦FP16ï¼Œè¿™ä¸€æ­¥è‡³å…³é‡è¦
    return blob.astype(dtype)

# --- é€‰æ‹©ä½¿ç”¨çš„é¢„å¤„ç†å‡½æ•° ---
# åˆ‡æ¢åˆ°æ–°çš„ç»ˆæä¼˜åŒ–ç‰ˆæœ¬ï¼
preprocess = preprocess_ultimate_optimized
# å¦‚æœéœ€è¦å¯¹æ¯”ï¼Œå¯ä»¥åˆ‡æ¢å›æ—§ç‰ˆ
# preprocess = lambda img: preprocess_optimized_numpy(img, dtype=np.float16)


def postprocess(output_tensor, original_width, original_height):
    """åå¤„ç†å‡½æ•°ä¿æŒä¸å˜"""
    pred_mask = np.argmax(output_tensor, axis=1).squeeze()
    vis_mask = (pred_mask * 255).astype(np.uint8)
    vis_mask_resized = cv2.resize(vis_mask, (original_width, original_height), interpolation=cv2.INTER_NEAREST)
    return vis_mask_resized

# --- æ‘„åƒå¤´æŠ“å–çº¿ç¨‹ (æ— ä¿®æ”¹) ---
def camera_capture_thread():
    print("æ­£åœ¨æ‰“å¼€æ‘„åƒå¤´...")
    cap = cv2.VideoCapture(CAMERA_INDEX, cv2.CAP_V4L2)
    if not cap.isOpened():
        print(f"é”™è¯¯: æ— æ³•æ‰“å¼€æ‘„åƒå¤´ç´¢å¼• {CAMERA_INDEX}.")
        return

    fourcc = cv2.VideoWriter_fourcc(*'MJPG')
    cap.set(cv2.CAP_PROP_FOURCC, fourcc)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, CAMERA_WIDTH)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, CAMERA_HEIGHT)
    cap.set(cv2.CAP_PROP_FPS, 30)
    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)

    actual_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    actual_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    actual_fps = cap.get(cv2.CAP_PROP_FPS)
    print("\n--- æ‘„åƒå¤´å®é™…å‚æ•° ---")
    print(f"åˆ†è¾¨ç‡: {actual_w}x{actual_h}, å¸§ç‡: {actual_fps}")
    print("---------------------------\n")

    while True:
        ret, frame = cap.read()
        if not ret:
            time.sleep(0.1)
            continue
        try:
            frame_queue.put_nowait(frame)
        except queue.Full:
            pass
    cap.release()

# --- æ¨ç†çº¿ç¨‹ (ä¿®æ”¹ä»¥ä½¿ç”¨æ–°é¢„å¤„ç†å‡½æ•°å¹¶æ‰“å°æ•°æ®ç±»å‹) ---
def inference_thread():
    global stats_data, data_lock
    print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
    model = InferSession(DEVICE_ID, MODEL_PATH)
    print("æ¨¡å‹åŠ è½½å®Œæˆã€‚")
    
    frame_count = 0
    total_times = {"preprocess": 0, "inference": 0, "postprocess": 0, "pipeline": 0}
    
    # å¢åŠ ä¸€ä¸ªå˜é‡æ¥ç¡®è®¤æ•°æ®ç±»å‹
    input_dtype_str = "N/A"

    print("\n=== å¼€å§‹æ€§èƒ½ç›‘æ§ (ç»ˆæä¼˜åŒ–ç‰ˆ: FP16 + blobFromImage) ===")
    print("æ¯20å¸§è¾“å‡ºä¸€æ¬¡è¯¦ç»†æ€§èƒ½åˆ†æ...")

    while True:
        try:
            frame = frame_queue.get(timeout=1)
        except queue.Empty:
            continue

        loop_start_time = time.time()
        cam_height, cam_width = frame.shape[:2]
        
        # --- é¢„å¤„ç†è®¡æ—¶ ---
        preprocess_start = time.time()
        # è°ƒç”¨æˆ‘ä»¬é€‰æ‹©çš„é¢„å¤„ç†å‡½æ•°
        # é»˜è®¤ä½¿ç”¨ preprocess_ultimate_optimized, å¹¶ä¼ å…¥ np.float16
        input_data = preprocess(frame, dtype=np.float16) 
        preprocess_time_ms = (time.time() - preprocess_start) * 1000
        
        # è®°å½•æ•°æ®ç±»å‹ï¼Œåªè®°å½•ä¸€æ¬¡
        if frame_count == 0:
            input_dtype_str = str(input_data.dtype)

        # --- æ¨ç†è®¡æ—¶ ---
        infer_start_time = time.time()
        outputs = model.infer([input_data])
        inference_time_ms = (time.time() - infer_start_time) * 1000
        
        # --- åå¤„ç†è®¡æ—¶ ---
        postprocess_start = time.time()
        lane_mask = postprocess(outputs[0], cam_width, cam_height)
        postprocess_time_ms = (time.time() - postprocess_start) * 1000
        
        pipeline_latency_ms = (time.time() - loop_start_time) * 1000
        
        # ç´¯è®¡ç»Ÿè®¡
        frame_count += 1
        total_times["preprocess"] += preprocess_time_ms
        total_times["inference"] += inference_time_ms
        total_times["postprocess"] += postprocess_time_ms
        total_times["pipeline"] += pipeline_latency_ms
        
        if frame_count % 20 == 0:
            avg_preprocess = total_times["preprocess"] / frame_count
            avg_inference = total_times["inference"] / frame_count
            avg_postprocess = total_times["postprocess"] / frame_count
            avg_pipeline = total_times["pipeline"] / frame_count
            
            print(f"\n--- ç¬¬{frame_count}å¸§æ€§èƒ½åˆ†æ (ç»ˆæä¼˜åŒ–ç‰ˆ) ---")
            print(f"è¾“å…¥å¸§: {cam_width}x{cam_height} -> æ¨¡å‹è¾“å…¥: {MODEL_WIDTH}x{MODEL_HEIGHT}")
            print(f"ğŸ”¥ æ¨¡å‹è¾“å…¥æ•°æ®ç±»å‹: {input_dtype_str}")
            print(f"")
            print(f"ã€é¢„å¤„ç†ã€‘: {preprocess_time_ms:.1f}ms   (å¹³å‡: {avg_preprocess:.1f}ms) âš¡ï¸")
            print(f"ã€æ¨  ç†ã€‘: {inference_time_ms:.1f}ms   (å¹³å‡: {avg_inference:.1f}ms) âš¡ï¸")
            print(f"ã€åå¤„ç†ã€‘: {postprocess_time_ms:.1f}ms   (å¹³å‡: {avg_postprocess:.1f}ms)")
            print(f"--------------------------------------------------")
            print(f"ã€æµæ°´çº¿æ€»å»¶è¿Ÿã€‘: {pipeline_latency_ms:.1f}ms (ç†è®ºFPS: {1000/pipeline_latency_ms:.1f})")
            print(f"ã€å¹³å‡æ€»å»¶è¿Ÿã€‘  : {avg_pipeline:.1f}ms (å¹³å‡FPS: {1000/avg_pipeline:.1f})")
            print(f"")

            # ç“¶é¢ˆåˆ†æ
            max_time = max(avg_preprocess, avg_inference, avg_postprocess)
            if max_time == avg_preprocess:
                print("ğŸ”´ å½“å‰ç“¶é¢ˆ: é¢„å¤„ç† (CPU)")
                print("   ğŸ¤” ä¼˜åŒ–å»ºè®®: å¦‚æœä»ç„¶æ˜¯ç“¶é¢ˆï¼Œè¯·è€ƒè™‘ä½¿ç”¨AIPPè¿›è¡Œç¡¬ä»¶é¢„å¤„ç†ã€‚")
            elif max_time == avg_inference:
                print("ğŸŸ¡ å½“å‰ç“¶é¢ˆ: NPUæ¨ç†")
                print("   âœ… ä¼˜åŒ–çŠ¶æ€: å·²ä½¿ç”¨FP16ï¼Œè‹¥éœ€æ›´å¿«å¯è€ƒè™‘INT8é‡åŒ–æˆ–æ›´è½»é‡æ¨¡å‹ã€‚")
            else:
                print("ğŸŸ¢ å½“å‰ç“¶é¢ˆ: åå¤„ç†")
                print("   âœ… ä¼˜åŒ–çŠ¶æ€: åå¤„ç†é€šå¸¸ä¸æ˜¯ä¸»è¦ç“¶é¢ˆã€‚")
            print("=" * 60)

        try:
            result_queue.put_nowait({
                "frame": frame, "mask": lane_mask,
                "latency": pipeline_latency_ms, "inference_time": inference_time_ms,
                "preprocess_time": preprocess_time_ms, "postprocess_time": postprocess_time_ms
            })
        except queue.Full:
            pass
        
        with data_lock:
            stats_data["pipeline_latency"] = f"{pipeline_latency_ms:.1f}"
            stats_data["inference_time"] = f"{inference_time_ms:.1f}"
            stats_data["preprocess_time"] = f"{preprocess_time_ms:.1f}"
            stats_data["postprocess_time"] = f"{postprocess_time_ms:.1f}"


# --- NPU/CPU ç›‘æ§çº¿ç¨‹ å’Œ Flaskéƒ¨åˆ† (æ— ä¿®æ”¹) ---
def system_monitor_loop():
    global stats_data, data_lock
    npu_error_printed = False
    
    while True:
        cpu = psutil.cpu_percent()
        mem = psutil.virtual_memory().percent
        with data_lock:
            stats_data["cpu_percent"] = f"{cpu:.1f}"
            stats_data["mem_percent"] = f"{mem:.1f}"

        try:
            result = subprocess.run([NPU_SMI_PATH, 'info'], capture_output=True, text=True, check=True, timeout=2)
            output = result.stdout
            util_match = re.search(r'NPU\s+Utilization\s*:\s*(\d+)\s*%', output)
            mem_match = re.search(r'Memory\s+Usage\s*:\s*([\d\.]+)\s*MiB\s*/\s*([\d\.]+)\s*MiB', output)
            npu_util = util_match.group(1) if util_match else "N/A"
            npu_mem = f"{float(mem_match.group(1)):.0f} / {float(mem_match.group(2)):.0f} MiB" if mem_match else "N/A"
            with data_lock:
                stats_data["npu_util"] = npu_util
                stats_data["npu_mem"] = npu_mem
            if npu_error_printed:
                print("âœ… NPUç›‘æ§å·²æ¢å¤æ­£å¸¸")
                npu_error_printed = False
        except Exception:
             if not npu_error_printed:
                 print(f"âŒ NPUç›‘æ§å¤±è´¥ï¼Œè¯·æ£€æŸ¥è·¯å¾„: {NPU_SMI_PATH}")
                 npu_error_printed = True
             with data_lock:
                 stats_data["npu_util"] = "Error"
                 stats_data["npu_mem"] = "Error"
        time.sleep(1)

HTML_TEMPLATE = """
<!DOCTYPE html>
<html>
<head>
    <title>è½¦é“çº¿æ£€æµ‹å®æ—¶æ¨ç†</title>
    <meta charset="UTF-8">
    <style>
        body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif; margin: 0; padding: 20px; background-color: #f4f7f6; color: #333; }
        .container { max-width: 1400px; margin: 0 auto; background-color: white; padding: 20px; border-radius: 10px; box-shadow: 0 4px 15px rgba(0,0,0,0.05); }
        h1 { text-align: center; color: #1a73e8; }
        .main-content { display: flex; flex-wrap: wrap; gap: 20px; }
        .video-container { flex: 3; min-width: 600px; }
        #videoStream { width: 100%; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); background-color: #eee; }
        .stats-container { flex: 1; min-width: 300px; background-color: #f8f9fa; padding: 20px; border-radius: 8px; }
        .stats-container h2 { margin-top: 0; border-bottom: 2px solid #e0e0e0; padding-bottom: 10px; color: #3c4043; }
        .stat-grid { display: grid; grid-template-columns: 1fr; gap: 15px; }
        .stat-card { background-color: #fff; padding: 15px; border-radius: 5px; border-left: 5px solid #1a73e8; box-shadow: 0 1px 3px rgba(0,0,0,0.08); display: flex; justify-content: space-between; align-items: center; }
        .stat-card.npu { border-left-color: #34a853; }
        .stat-card.cpu { border-left-color: #fbbc05; }
        .stat-label { font-size: 14px; color: #5f6368; }
        .stat-value { font-size: 18px; font-weight: 600; color: #202124; }
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸš— è½¦é“çº¿æ£€æµ‹å®æ—¶æ¨ç†ç›‘æ§ (ç»ˆæä¼˜åŒ–ç‰ˆ: FP16 + blobFromImage)</h1>
        <div class="main-content">
            <div class="video-container">
                <img id="videoStream" src="{{ url_for('video_feed') }}" alt="Live Inference Stream">
            </div>
            <div class="stats-container">
                <h2>ğŸ“Š æ€§èƒ½ç›‘æ§</h2>
                <div class="stat-grid">
                    <div class="stat-card">
                        <span class="stat-label">æ˜¾ç¤ºå¸§ç‡ (FPS)</span>
                        <span id="fps" class="stat-value">--</span>
                    </div>
                    <div class="stat-card">
                        <span class="stat-label">å¤„ç†æµæ°´çº¿å»¶è¿Ÿ (ms)</span>
                        <span id="pipeline_latency" class="stat-value">--</span>
                    </div>
                    <div class="stat-card">
                        <span class="stat-label">æ¨¡å‹æ¨ç†è€—æ—¶ (ms)</span>
                        <span id="inference_time" class="stat-value">--</span>
                    </div>
                    <div class="stat-card">
                        <span class="stat-label">é¢„å¤„ç†è€—æ—¶ (ms)</span>
                        <span id="preprocess_time" class="stat-value">--</span>
                    </div>
                    <div class="stat-card">
                        <span class="stat-label">åå¤„ç†è€—æ—¶ (ms)</span>
                        <span id="postprocess_time" class="stat-value">--</span>
                    </div>
                    <div class="stat-card npu">
                        <span class="stat-label">NPU åˆ©ç”¨ç‡ (%)</span>
                        <span id="npu_util" class="stat-value">--</span>
                    </div>
                    <div class="stat-card npu">
                        <span class="stat-label">NPU å†…å­˜å ç”¨</span>
                        <span id="npu_mem" class="stat-value">--</span>
                    </div>
                    <div class="stat-card cpu">
                        <span class="stat-label">CPU åˆ©ç”¨ç‡ (%)</span>
                        <span id="cpu_percent" class="stat-value">--</span>
                    </div>
                    <div class="stat-card cpu">
                        <span class="stat-label">ç³»ç»Ÿå†…å­˜å ç”¨ (%)</span>
                        <span id="mem_percent" class="stat-value">--</span>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <script>
        function refreshStats() {
            fetch('/stats')
            .then(response => response.json())
            .then(data => {
                document.getElementById('fps').textContent = data.fps;
                document.getElementById('pipeline_latency').textContent = data.pipeline_latency + ' ms';
                document.getElementById('inference_time').textContent = data.inference_time + ' ms';
                document.getElementById('preprocess_time').textContent = data.preprocess_time + ' ms';
                document.getElementById('postprocess_time').textContent = data.postprocess_time + ' ms';
                document.getElementById('npu_util').textContent = data.npu_util + ' %';
                document.getElementById('npu_mem').textContent = data.npu_mem;
                document.getElementById('cpu_percent').textContent = data.cpu_percent + ' %';
                document.getElementById('mem_percent').textContent = data.mem_percent + ' %';
            })
            .catch(error => console.error('è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥:', error));
        }
        window.onload = function() {
            refreshStats();
            setInterval(refreshStats, 1000);
        };
    </script>
</body>
</html>
"""

@app.route("/")
def index():
    return render_template_string(HTML_TEMPLATE)

@app.route("/video_feed")
def video_feed():
    def generate():
        global stats_data, data_lock
        frame_count = 0
        start_time = time.time()
        while True:
            try:
                result = result_queue.get(timeout=1)
            except queue.Empty:
                continue
            frame = result["frame"]
            mask = result["mask"]
            green_overlay = np.zeros_like(frame, dtype=np.uint8)
            green_overlay[mask > 0] = [0, 255, 0]
            vis_frame = cv2.addWeighted(frame, 1.0, green_overlay, 0.5, 0)
            frame_count += 1
            elapsed = time.time() - start_time
            if elapsed > 1.0:
                fps = frame_count / elapsed
                with data_lock:
                    stats_data["fps"] = f"{fps:.1f}"
                frame_count = 0
                start_time = time.time()
            with data_lock:
                fps_text = f"Display FPS: {stats_data['fps']}"
                latency_text = f"Latency: {stats_data['pipeline_latency']} ms"
            cv2.putText(vis_frame, fps_text, (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
            cv2.putText(vis_frame, latency_text, (15, 65), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
            (flag, encodedImage) = cv2.imencode(".jpg", vis_frame)
            if not flag:
                continue
            yield(b'--frame\r\n' b'Content-Type: image/jpeg\r\n\r\n' + 
                  bytearray(encodedImage) + b'\r\n')
    return Response(generate(), mimetype="multipart/x-mixed-replace; boundary=frame")

@app.route("/stats")
def stats():
    with data_lock:
        return jsonify(stats_data)

if __name__ == '__main__':
    print("ğŸš€ è½¦é“çº¿æ£€æµ‹å®æ—¶æ¨ç†ç³»ç»Ÿå¯åŠ¨ (ç»ˆæä¼˜åŒ–ç‰ˆ: FP16 + blobFromImage)")
    print("=" * 60)
    print(f"ğŸ“· æ‘„åƒå¤´é…ç½®: {CAMERA_WIDTH}x{CAMERA_HEIGHT} @ {CAMERA_INDEX}")
    print(f"ğŸ§  æ¨¡å‹é…ç½®: {MODEL_PATH}")
    print(f"   - è¾“å…¥å°ºå¯¸: {MODEL_WIDTH}x{MODEL_HEIGHT}, è®¾å¤‡ID: {DEVICE_ID}")
    print(f"âš¡ ä¼˜åŒ–ç‰¹æ€§:")
    print(f"   - âœ… é¢„å¤„ç†: cv2.dnn.blobFromImage (é«˜é€ŸCPUå¤„ç†)")
    print(f"   - âœ… æ•°æ®ç±»å‹: FP16 (åŠ é€Ÿæ•°æ®ä¼ è¾“å’ŒNPUæ¨ç†)")
    print(f"ğŸ’» ç³»ç»Ÿä¿¡æ¯:")
    print(f"   - CPUæ ¸å¿ƒ: {psutil.cpu_count()}, å†…å­˜: {psutil.virtual_memory().total / (1024**3):.1f} GB")
    print(f"   - NPUå·¥å…·è·¯å¾„: {NPU_SMI_PATH}")
    print("=" * 60)
    
    Thread(target=camera_capture_thread, daemon=True).start()
    Thread(target=inference_thread, daemon=True).start()
    Thread(target=system_monitor_loop, daemon=True).start()
    
    print("WebæœåŠ¡å™¨å·²å¯åŠ¨ã€‚è¯·åœ¨æµè§ˆå™¨ä¸­è®¿é—®: http://<Your_Atlas_IP>:8000")
    print("ç»ˆç«¯å°†æ˜¾ç¤ºè¯¦ç»†çš„æ€§èƒ½åˆ†æä¿¡æ¯...")
    app.run(host='0.0.0.0', port=8000, threaded=True)