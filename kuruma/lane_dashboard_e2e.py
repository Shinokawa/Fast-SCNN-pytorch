import os
import cv2
import time
import numpy as np
from threading import Thread, Lock
import queue # å¼•å…¥é˜Ÿåˆ—æ¨¡å—
import psutil
import subprocess
import re

from ais_bench.infer.interface import InferSession
from flask import Flask, Response, render_template_string, jsonify

# --- Flask App å’Œå…¨å±€å˜é‡ ---
app = Flask(__name__)
frame_queue = queue.Queue(maxsize=1)
result_queue = queue.Queue(maxsize=1)
stats_data = {
    "fps": "0.0", "pipeline_latency": "0.0", "inference_time": "0.0",
    "preprocess_time": "0.0", "postprocess_time": "0.0",
    "cpu_percent": "0.0", "mem_percent": "0.0", "npu_util": "N/A", "npu_mem": "N/A"
}
data_lock = Lock()

# --- å¯é…ç½®å¸¸é‡ ---
DEVICE_ID = 0
MODEL_PATH = "./weights/fast_scnn_640_aippe2e.om"  # ç«¯åˆ°ç«¯æ¨¡å‹è·¯å¾„
MODEL_WIDTH = 640
MODEL_HEIGHT = 640
CAMERA_INDEX = 0
CAMERA_WIDTH = 640
CAMERA_HEIGHT = 640
NPU_SMI_PATH = "/usr/local/Ascend/driver/tools/npu-smi"

# --------------------------------------------------------------------------
# --- ğŸš€ğŸš€ğŸš€ ç«¯åˆ°ç«¯æ¨¡å‹é¢„å¤„ç†å‡½æ•° - æç®€ç‰ˆæœ¬ï¼ğŸš€ğŸš€ğŸš€ ---
# --------------------------------------------------------------------------

def preprocess_end_to_end(img_bgr, dtype=np.float32):
    """
    ç«¯åˆ°ç«¯æ¨¡å‹é¢„å¤„ç†å‡½æ•° - æç®€ç‰ˆæœ¬ï¼
    
    ç”±äºä½¿ç”¨äº†ç«¯åˆ°ç«¯æ¨¡å‹ï¼Œé¢„å¤„ç†æ“ä½œè¢«é›†æˆåˆ°äº†æ¨¡å‹å†…éƒ¨ï¼Œ
    è¿™é‡Œåªéœ€è¦ç®€å•çš„æ ¼å¼è½¬æ¢ï¼Œå¤§å¤§æå‡äº†æ€§èƒ½ï¼
    
    è¾“å…¥: BGRå›¾åƒ (H, W, 3) [0-255]
    è¾“å‡º: æ¨¡å‹è¾“å…¥å¼ é‡ (1, 3, 640, 640) [0-255] åŸå§‹åƒç´ å€¼
    """
    # ä»…éœ€ç®€å•çš„resize + æ ¼å¼è½¬æ¢ï¼Œä¿æŒåŸå§‹åƒç´ å€¼ [0-255]
    img_resized = cv2.resize(img_bgr, (MODEL_WIDTH, MODEL_HEIGHT), interpolation=cv2.INTER_LINEAR)
    
    # è½¬æ¢ä¸ºRGBï¼ˆç«¯åˆ°ç«¯æ¨¡å‹å†…éƒ¨ä¼šå¤„ç†ï¼‰
    img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)
    
    # è½¬æ¢ä¸ºCHWæ ¼å¼å¹¶æ·»åŠ batchç»´åº¦
    img_transposed = np.transpose(img_rgb, (2, 0, 1))  # HWC -> CHW
    input_data = np.ascontiguousarray(img_transposed[np.newaxis, :, :, :], dtype=dtype)
    
    return input_data

def postprocess(output_tensor, original_width, original_height):
    """åå¤„ç†å‡½æ•° - ä¸åŸç‰ˆä¿æŒä¸€è‡´"""
    pred_mask = np.argmax(output_tensor, axis=1).squeeze()
    vis_mask = (pred_mask * 255).astype(np.uint8)
    vis_mask_resized = cv2.resize(vis_mask, (original_width, original_height), interpolation=cv2.INTER_NEAREST)
    return vis_mask_resized

# --- æ‘„åƒå¤´æŠ“å–çº¿ç¨‹ (æ— ä¿®æ”¹) ---
def camera_capture_thread():
    print("æ­£åœ¨æ‰“å¼€æ‘„åƒå¤´...")
    cap = cv2.VideoCapture(CAMERA_INDEX, cv2.CAP_V4L2)
    if not cap.isOpened():
        print(f"é”™è¯¯: æ— æ³•æ‰“å¼€æ‘„åƒå¤´ç´¢å¼• {CAMERA_INDEX}.")
        return

    fourcc = cv2.VideoWriter_fourcc(*'MJPG')
    cap.set(cv2.CAP_PROP_FOURCC, fourcc)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, CAMERA_WIDTH)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, CAMERA_HEIGHT)
    cap.set(cv2.CAP_PROP_FPS, 30)
    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)

    actual_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    actual_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    actual_fps = cap.get(cv2.CAP_PROP_FPS)
    print("\n--- æ‘„åƒå¤´å®é™…å‚æ•° ---")
    print(f"åˆ†è¾¨ç‡: {actual_w}x{actual_h}, å¸§ç‡: {actual_fps}")
    print("---------------------------\n")

    while True:
        ret, frame = cap.read()
        if not ret:
            time.sleep(0.1)
            continue
        try:
            frame_queue.put_nowait(frame)
        except queue.Full:
            pass
    cap.release()

# --- æ¨ç†çº¿ç¨‹ (ä½¿ç”¨ç«¯åˆ°ç«¯æ¨¡å‹) ---
def inference_thread():
    global stats_data, data_lock
    print("æ­£åœ¨åŠ è½½ç«¯åˆ°ç«¯æ¨¡å‹...")
    model = InferSession(DEVICE_ID, MODEL_PATH)
    print("ç«¯åˆ°ç«¯æ¨¡å‹åŠ è½½å®Œæˆã€‚")
    
    frame_count = 0
    total_times = {"preprocess": 0, "inference": 0, "postprocess": 0, "pipeline": 0}
    
    # è®°å½•æ•°æ®ç±»å‹
    input_dtype_str = "N/A"

    print("\n=== ğŸš€ ç«¯åˆ°ç«¯æ¨¡å‹æ€§èƒ½ç›‘æ§å¼€å§‹ ğŸš€ ===")
    print("ğŸ’¡ é¢„å¤„ç†å·²å†…ç½®åˆ°NPUä¸­ï¼Œé¢„æœŸæ€§èƒ½å¤§å¹…æå‡ï¼")
    print("æ¯20å¸§è¾“å‡ºä¸€æ¬¡è¯¦ç»†æ€§èƒ½åˆ†æ...")

    while True:
        try:
            frame = frame_queue.get(timeout=1)
        except queue.Empty:
            continue

        loop_start_time = time.time()
        cam_height, cam_width = frame.shape[:2]
        
        # --- æç®€é¢„å¤„ç†è®¡æ—¶ ---
        preprocess_start = time.time()
        # ä½¿ç”¨ç«¯åˆ°ç«¯é¢„å¤„ç†å‡½æ•°ï¼Œåªéœ€ç®€å•æ ¼å¼è½¬æ¢
        input_data = preprocess_end_to_end(frame, dtype=np.float32)
        preprocess_time_ms = (time.time() - preprocess_start) * 1000
        
        # è®°å½•æ•°æ®ç±»å‹
        if frame_count == 0:
            input_dtype_str = str(input_data.dtype)

        # --- NPUç«¯åˆ°ç«¯æ¨ç†è®¡æ—¶ ---
        infer_start_time = time.time()
        outputs = model.infer([input_data])
        inference_time_ms = (time.time() - infer_start_time) * 1000
        
        # --- åå¤„ç†è®¡æ—¶ ---
        postprocess_start = time.time()
        lane_mask = postprocess(outputs[0], cam_width, cam_height)
        postprocess_time_ms = (time.time() - postprocess_start) * 1000
        
        pipeline_latency_ms = (time.time() - loop_start_time) * 1000
        
        # ç´¯è®¡ç»Ÿè®¡
        frame_count += 1
        total_times["preprocess"] += preprocess_time_ms
        total_times["inference"] += inference_time_ms
        total_times["postprocess"] += postprocess_time_ms
        total_times["pipeline"] += pipeline_latency_ms
        
        if frame_count % 20 == 0:
            avg_preprocess = total_times["preprocess"] / frame_count
            avg_inference = total_times["inference"] / frame_count
            avg_postprocess = total_times["postprocess"] / frame_count
            avg_pipeline = total_times["pipeline"] / frame_count
            
            print(f"\n--- ğŸš€ ç¬¬{frame_count}å¸§æ€§èƒ½åˆ†æ (ç«¯åˆ°ç«¯æ¨¡å‹) ---")
            print(f"è¾“å…¥å¸§: {cam_width}x{cam_height} -> æ¨¡å‹è¾“å…¥: {MODEL_WIDTH}x{MODEL_HEIGHT}")
            print(f"ğŸ¯ æ¨¡å‹è¾“å…¥æ•°æ®ç±»å‹: {input_dtype_str} (åŸå§‹åƒç´ å€¼ 0-255)")
            print(f"âœ¨ å†…ç½®NPUæ“ä½œ: resize + å½’ä¸€åŒ– + è¯­ä¹‰åˆ†å‰²æ¨ç†")
            print(f"")
            print(f"ã€CPUé¢„å¤„ç†ã€‘: {preprocess_time_ms:.1f}ms   (å¹³å‡: {avg_preprocess:.1f}ms) âš¡ï¸")
            print(f"ã€NPUç«¯åˆ°ç«¯æ¨ç†ã€‘: {inference_time_ms:.1f}ms   (å¹³å‡: {avg_inference:.1f}ms) ğŸš€")
            print(f"ã€CPUåå¤„ç†ã€‘: {postprocess_time_ms:.1f}ms   (å¹³å‡: {avg_postprocess:.1f}ms)")
            print(f"--------------------------------------------------")
            print(f"ã€æµæ°´çº¿æ€»å»¶è¿Ÿã€‘: {pipeline_latency_ms:.1f}ms (ç†è®ºFPS: {1000/pipeline_latency_ms:.1f})")
            print(f"ã€å¹³å‡æ€»å»¶è¿Ÿã€‘  : {avg_pipeline:.1f}ms (å¹³å‡FPS: {1000/avg_pipeline:.1f})")
            print(f"")

            # æ€§èƒ½åˆ†æ
            print("ğŸ¯ ç«¯åˆ°ç«¯æ¨¡å‹ä¼˜åŠ¿åˆ†æ:")
            print(f"   âœ… CPUé¢„å¤„ç†è´Ÿè½½: {avg_preprocess:.1f}ms (ä»…æ ¼å¼è½¬æ¢)")
            print(f"   ğŸš€ NPUé›†æˆå¤„ç†: {avg_inference:.1f}ms (resize+å½’ä¸€åŒ–+æ¨ç†)")
            print(f"   ğŸ’¾ æ•°æ®ä¼ è¾“ä¼˜åŒ–: å‡å°‘CPU-NPUæ‹·è´å¼€é”€")
            
            # ç“¶é¢ˆåˆ†æ
            max_time = max(avg_preprocess, avg_inference, avg_postprocess)
            if max_time == avg_preprocess:
                print("ğŸ”´ å½“å‰ç“¶é¢ˆ: CPUé¢„å¤„ç†")
                print("   ğŸ¤” å¼‚å¸¸æƒ…å†µ: ç«¯åˆ°ç«¯æ¨¡å‹é¢„å¤„ç†åº”è¯¥å¾ˆå¿«ï¼Œè¯·æ£€æŸ¥é…ç½®")
            elif max_time == avg_inference:
                print("ğŸŸ¢ å½“å‰ç“¶é¢ˆ: NPUæ¨ç† (æ­£å¸¸)")
                print("   âœ… ä¼˜åŒ–çŠ¶æ€: NPUæ‰¿æ‹…äº†å¤§éƒ¨åˆ†è®¡ç®—è´Ÿè½½ï¼Œè¿™æ˜¯æœ€ä¼˜çŠ¶æ€")
            else:
                print("ğŸŸ¡ å½“å‰ç“¶é¢ˆ: åå¤„ç†")
                print("   ğŸ’¡ ä¼˜åŒ–å»ºè®®: å¯è€ƒè™‘å°†åå¤„ç†ä¹Ÿé›†æˆåˆ°æ¨¡å‹ä¸­")
            
            # ä¸ä¼ ç»Ÿæ–¹æ¡ˆå¯¹æ¯”
            traditional_preprocess_estimate = avg_preprocess * 3  # ä¼°ç®—ä¼ ç»Ÿé¢„å¤„ç†æ—¶é—´
            total_savings = traditional_preprocess_estimate - avg_preprocess
            print(f"")
            print(f"ğŸ“Š ä¸ä¼ ç»Ÿæ–¹æ¡ˆå¯¹æ¯”:")
            print(f"   ä¼ ç»Ÿé¢„å¤„ç†ä¼°ç®—: {traditional_preprocess_estimate:.1f}ms")
            print(f"   ç«¯åˆ°ç«¯é¢„å¤„ç†: {avg_preprocess:.1f}ms")
            print(f"   ğŸ‰ é¢„å¤„ç†æå‡: {total_savings:.1f}ms ({total_savings/traditional_preprocess_estimate*100:.1f}%)")
            print("=" * 60)

        try:
            result_queue.put_nowait({
                "frame": frame, "mask": lane_mask,
                "latency": pipeline_latency_ms, "inference_time": inference_time_ms,
                "preprocess_time": preprocess_time_ms, "postprocess_time": postprocess_time_ms
            })
        except queue.Full:
            pass
        
        with data_lock:
            stats_data["pipeline_latency"] = f"{pipeline_latency_ms:.1f}"
            stats_data["inference_time"] = f"{inference_time_ms:.1f}"
            stats_data["preprocess_time"] = f"{preprocess_time_ms:.1f}"
            stats_data["postprocess_time"] = f"{postprocess_time_ms:.1f}"

# --- NPU/CPU ç›‘æ§çº¿ç¨‹ (æ— ä¿®æ”¹) ---
def system_monitor_loop():
    global stats_data, data_lock
    npu_error_printed = False
    
    while True:
        cpu = psutil.cpu_percent()
        mem = psutil.virtual_memory().percent
        with data_lock:
            stats_data["cpu_percent"] = f"{cpu:.1f}"
            stats_data["mem_percent"] = f"{mem:.1f}"

        try:
            result = subprocess.run([NPU_SMI_PATH, 'info'], capture_output=True, text=True, check=True, timeout=2)
            output = result.stdout
            util_match = re.search(r'NPU\s+Utilization\s*:\s*(\d+)\s*%', output)
            mem_match = re.search(r'Memory\s+Usage\s*:\s*([\d\.]+)\s*MiB\s*/\s*([\d\.]+)\s*MiB', output)
            npu_util = util_match.group(1) if util_match else "N/A"
            npu_mem = f"{float(mem_match.group(1)):.0f} / {float(mem_match.group(2)):.0f} MiB" if mem_match else "N/A"
            with data_lock:
                stats_data["npu_util"] = npu_util
                stats_data["npu_mem"] = npu_mem
            if npu_error_printed:
                print("âœ… NPUç›‘æ§å·²æ¢å¤æ­£å¸¸")
                npu_error_printed = False
        except Exception:
             if not npu_error_printed:
                 print(f"âŒ NPUç›‘æ§å¤±è´¥ï¼Œè¯·æ£€æŸ¥è·¯å¾„: {NPU_SMI_PATH}")
                 npu_error_printed = True
             with data_lock:
                 stats_data["npu_util"] = "Error"
                 stats_data["npu_mem"] = "Error"
        time.sleep(1)

# --- HTMLæ¨¡æ¿ (æ›´æ–°æ ‡é¢˜) ---
HTML_TEMPLATE = """
<!DOCTYPE html>
<html>
<head>
    <title>è½¦é“çº¿æ£€æµ‹ç«¯åˆ°ç«¯å®æ—¶æ¨ç†</title>
    <meta charset="UTF-8">
    <style>
        body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif; margin: 0; padding: 20px; background-color: #f4f7f6; color: #333; }
        .container { max-width: 1400px; margin: 0 auto; background-color: white; padding: 20px; border-radius: 10px; box-shadow: 0 4px 15px rgba(0,0,0,0.05); }
        h1 { text-align: center; color: #1a73e8; }
        .main-content { display: flex; flex-wrap: wrap; gap: 20px; }
        .video-container { flex: 3; min-width: 600px; }
        #videoStream { width: 100%; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); background-color: #eee; }
        .stats-container { flex: 1; min-width: 300px; background-color: #f8f9fa; padding: 20px; border-radius: 8px; }
        .stats-container h2 { margin-top: 0; border-bottom: 2px solid #e0e0e0; padding-bottom: 10px; color: #3c4043; }
        .stat-grid { display: grid; grid-template-columns: 1fr; gap: 15px; }
        .stat-card { background-color: #fff; padding: 15px; border-radius: 5px; border-left: 5px solid #1a73e8; box-shadow: 0 1px 3px rgba(0,0,0,0.08); display: flex; justify-content: space-between; align-items: center; }
        .stat-card.npu { border-left-color: #34a853; }
        .stat-card.cpu { border-left-color: #fbbc05; }
        .stat-card.e2e { border-left-color: #ea4335; }
        .stat-label { font-size: 14px; color: #5f6368; }
        .stat-value { font-size: 18px; font-weight: 600; color: #202124; }
        .optimization-badge { background: linear-gradient(45deg, #ff6b6b, #4ecdc4); color: white; padding: 5px 10px; border-radius: 15px; font-size: 12px; font-weight: bold; margin-left: 10px; }
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸš€ è½¦é“çº¿æ£€æµ‹ç«¯åˆ°ç«¯å®æ—¶æ¨ç†ç›‘æ§<span class="optimization-badge">ç«¯åˆ°ç«¯NPUåŠ é€Ÿ</span></h1>
        <div class="main-content">
            <div class="video-container">
                <img id="videoStream" src="{{ url_for('video_feed') }}" alt="End-to-End Live Inference Stream">
            </div>
            <div class="stats-container">
                <h2>ğŸ“Š ç«¯åˆ°ç«¯æ€§èƒ½ç›‘æ§</h2>
                <div class="stat-grid">
                    <div class="stat-card">
                        <span class="stat-label">æ˜¾ç¤ºå¸§ç‡ (FPS)</span>
                        <span id="fps" class="stat-value">--</span>
                    </div>
                    <div class="stat-card e2e">
                        <span class="stat-label">ç«¯åˆ°ç«¯æµæ°´çº¿å»¶è¿Ÿ (ms)</span>
                        <span id="pipeline_latency" class="stat-value">--</span>
                    </div>
                    <div class="stat-card e2e">
                        <span class="stat-label">NPUç«¯åˆ°ç«¯æ¨ç† (ms)</span>
                        <span id="inference_time" class="stat-value">--</span>
                    </div>
                    <div class="stat-card">
                        <span class="stat-label">CPUé¢„å¤„ç† (ms)</span>
                        <span id="preprocess_time" class="stat-value">--</span>
                    </div>
                    <div class="stat-card">
                        <span class="stat-label">åå¤„ç†è€—æ—¶ (ms)</span>
                        <span id="postprocess_time" class="stat-value">--</span>
                    </div>
                    <div class="stat-card npu">
                        <span class="stat-label">NPU åˆ©ç”¨ç‡ (%)</span>
                        <span id="npu_util" class="stat-value">--</span>
                    </div>
                    <div class="stat-card npu">
                        <span class="stat-label">NPU å†…å­˜å ç”¨</span>
                        <span id="npu_mem" class="stat-value">--</span>
                    </div>
                    <div class="stat-card cpu">
                        <span class="stat-label">CPU åˆ©ç”¨ç‡ (%)</span>
                        <span id="cpu_percent" class="stat-value">--</span>
                    </div>
                    <div class="stat-card cpu">
                        <span class="stat-label">ç³»ç»Ÿå†…å­˜å ç”¨ (%)</span>
                        <span id="mem_percent" class="stat-value">--</span>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <script>
        function refreshStats() {
            fetch('/stats')
            .then(response => response.json())
            .then(data => {
                document.getElementById('fps').textContent = data.fps;
                document.getElementById('pipeline_latency').textContent = data.pipeline_latency + ' ms';
                document.getElementById('inference_time').textContent = data.inference_time + ' ms';
                document.getElementById('preprocess_time').textContent = data.preprocess_time + ' ms';
                document.getElementById('postprocess_time').textContent = data.postprocess_time + ' ms';
                document.getElementById('npu_util').textContent = data.npu_util + ' %';
                document.getElementById('npu_mem').textContent = data.npu_mem;
                document.getElementById('cpu_percent').textContent = data.cpu_percent + ' %';
                document.getElementById('mem_percent').textContent = data.mem_percent + ' %';
            })
            .catch(error => console.error('è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥:', error));
        }
        window.onload = function() {
            refreshStats();
            setInterval(refreshStats, 1000);
        };
    </script>
</body>
</html>
"""

# --- Flaskè·¯ç”± (æ— ä¿®æ”¹) ---
@app.route("/")
def index():
    return render_template_string(HTML_TEMPLATE)

@app.route("/video_feed")
def video_feed():
    def generate():
        global stats_data, data_lock
        frame_count = 0
        start_time = time.time()
        while True:
            try:
                result = result_queue.get(timeout=1)
            except queue.Empty:
                continue
            frame = result["frame"]
            mask = result["mask"]
            green_overlay = np.zeros_like(frame, dtype=np.uint8)
            green_overlay[mask > 0] = [0, 255, 0]
            vis_frame = cv2.addWeighted(frame, 1.0, green_overlay, 0.5, 0)
            frame_count += 1
            elapsed = time.time() - start_time
            if elapsed > 1.0:
                fps = frame_count / elapsed
                with data_lock:
                    stats_data["fps"] = f"{fps:.1f}"
                frame_count = 0
                start_time = time.time()
            with data_lock:
                fps_text = f"E2E FPS: {stats_data['fps']}"
                latency_text = f"E2E Latency: {stats_data['pipeline_latency']} ms"
            cv2.putText(vis_frame, fps_text, (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
            cv2.putText(vis_frame, latency_text, (15, 65), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
            (flag, encodedImage) = cv2.imencode(".jpg", vis_frame)
            if not flag:
                continue
            yield(b'--frame\r\n' b'Content-Type: image/jpeg\r\n\r\n' + 
                  bytearray(encodedImage) + b'\r\n')
    return Response(generate(), mimetype="multipart/x-mixed-replace; boundary=frame")

@app.route("/stats")
def stats():
    with data_lock:
        return jsonify(stats_data)

if __name__ == '__main__':
    print("ğŸš€ è½¦é“çº¿æ£€æµ‹ç«¯åˆ°ç«¯å®æ—¶æ¨ç†ç³»ç»Ÿå¯åŠ¨")
    print("=" * 60)
    print(f"ğŸ“· æ‘„åƒå¤´é…ç½®: {CAMERA_WIDTH}x{CAMERA_HEIGHT} @ {CAMERA_INDEX}")
    print(f"ğŸ§  ç«¯åˆ°ç«¯æ¨¡å‹é…ç½®: {MODEL_PATH}")
    print(f"   - è¾“å…¥å°ºå¯¸: {MODEL_WIDTH}x{MODEL_HEIGHT}, è®¾å¤‡ID: {DEVICE_ID}")
    print(f"âš¡ ç«¯åˆ°ç«¯ä¼˜åŒ–ç‰¹æ€§:")
    print(f"   - âœ… NPUå†…ç½®é¢„å¤„ç†: resize + å½’ä¸€åŒ–åœ¨NPUä¸­å®Œæˆ")
    print(f"   - âœ… æç®€CPUå¤„ç†: ä»…æ ¼å¼è½¬æ¢ï¼Œæ— å¤æ‚è®¡ç®—")
    print(f"   - âœ… æ•°æ®ç±»å‹ä¼˜åŒ–: åŸå§‹åƒç´ å€¼ç›´æ¥è¾“å…¥ï¼Œå‡å°‘è½¬æ¢å¼€é”€")
    print(f"   - âœ… å†…å­˜ä¼˜åŒ–: å‡å°‘CPU-NPUæ•°æ®ä¼ è¾“")
    print(f"ğŸ’» ç³»ç»Ÿä¿¡æ¯:")
    print(f"   - CPUæ ¸å¿ƒ: {psutil.cpu_count()}, å†…å­˜: {psutil.virtual_memory().total / (1024**3):.1f} GB")
    print(f"   - NPUå·¥å…·è·¯å¾„: {NPU_SMI_PATH}")
    print("=" * 60)
    print("ğŸ’¡ é¢„æœŸæ€§èƒ½æå‡:")
    print("   - ğŸ¯ é¢„å¤„ç†æ—¶é—´: å‡å°‘70%+")
    print("   - ğŸš€ æ€»ä½“FPS: æå‡2-3å€")
    print("   - ğŸ’¾ CPUä½¿ç”¨ç‡: æ˜¾è‘—é™ä½")
    print("   - âš¡ ç«¯åˆ°ç«¯å»¶è¿Ÿ: <10ms")
    print("=" * 60)
    
    Thread(target=camera_capture_thread, daemon=True).start()
    Thread(target=inference_thread, daemon=True).start()
    Thread(target=system_monitor_loop, daemon=True).start()
    
    print("WebæœåŠ¡å™¨å·²å¯åŠ¨ã€‚è¯·åœ¨æµè§ˆå™¨ä¸­è®¿é—®: http://<Your_Atlas_IP>:8000")
    print("ç»ˆç«¯å°†æ˜¾ç¤ºè¯¦ç»†çš„ç«¯åˆ°ç«¯æ€§èƒ½åˆ†æä¿¡æ¯...")
    app.run(host='0.0.0.0', port=8000, threaded=True)
