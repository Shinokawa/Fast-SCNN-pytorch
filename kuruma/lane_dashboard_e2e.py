import os
import cv2
import time
import numpy as np
from threading import Thread, Lock
import queue
import psutil
import subprocess
import re

from ais_bench.infer.interface import InferSession
from flask import Flask, Response, render_template_string, jsonify

# --- Flask App å’Œå…¨å±€å˜é‡ ---
app = Flask(__name__)
frame_queue = queue.Queue(maxsize=1)
result_queue = queue.Queue(maxsize=1)
stats_data = {
    "fps": "0.0", "pipeline_latency": "0.0", "inference_time": "0.0",
    "preprocess_time": "0.0", "postprocess_time": "0.0",
    "cpu_percent": "0.0", "mem_percent": "0.0", "npu_util": "N/A", "npu_mem": "N/A"
}
data_lock = Lock()

# --- ã€å…³é”®ã€‘å¯é…ç½®å¸¸é‡ ---
DEVICE_ID = 0
# ä½¿ç”¨æ‚¨çš„æ–°æ¨¡å‹è·¯å¾„
MODEL_PATH = "./weights/fast_scnn_480x640_e2e_fp16_input.om" 
# æ¨¡å‹è¾“å…¥å°ºå¯¸ä¸æ‘„åƒå¤´è¾“å‡ºå®Œå…¨åŒ¹é…
MODEL_WIDTH = 640
MODEL_HEIGHT = 480
CAMERA_INDEX = 0
CAMERA_WIDTH = 640
CAMERA_HEIGHT = 480
NPU_SMI_PATH = "/usr/local/Ascend/driver/tools/npu-smi"

# ---------------------------------------------------------------------------------
# --- ğŸš€ğŸš€ğŸš€ æç®€é¢„å¤„ç† (åˆ†è¾¨ç‡åŒ¹é…) ğŸš€ğŸš€ğŸš€ ---
# ---------------------------------------------------------------------------------

def preprocess_matched_resolution(img_bgr, dtype=np.float16):
    """
    å½“æ¨¡å‹è¾“å…¥åˆ†è¾¨ç‡ä¸æ‘„åƒå¤´è¾“å‡ºå®Œå…¨åŒ¹é…æ—¶ï¼Œé¢„å¤„ç†å¼€é”€æœ€å°åŒ–ã€‚
    CPUåªè´Ÿè´£æœ€åŸºæœ¬çš„æ•°æ®æ ¼å¼å’Œç±»å‹è½¬æ¢ã€‚
    """
    # 1. è½¬æ¢é¢œè‰²é€šé“ (BGR -> RGB)
    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
    
    # 2. è½¬æ¢æ•°æ®ç±»å‹ (uint8 -> float16)
    img_typed = img_rgb.astype(dtype)
    
    # 3. è½¬æ¢ä¸ºCHWæ ¼å¼å¹¶æ·»åŠ batchç»´åº¦
    img_transposed = np.transpose(img_typed, (2, 0, 1))
    return np.ascontiguousarray(img_transposed[np.newaxis, :, :, :])

# ---------------------------------------------------------------------------------
# --- ğŸš€ğŸš€ğŸš€ æç®€åå¤„ç† (æ— éœ€è£å‰ª) ğŸš€ğŸš€ğŸš€ ---
# ---------------------------------------------------------------------------------

def postprocess_matched_resolution(output_tensor, original_width, original_height):
    """
    ç”±äºè¾“å…¥å’Œæ¨¡å‹å°ºå¯¸åŒ¹é…ï¼Œè¾“å‡ºä¹Ÿç›´æ¥å¯¹åº”åŸå§‹å›¾åƒï¼Œæ— éœ€è£å‰ªã€‚
    """
    # 1. Argmaxè·å–åˆ†å‰²æ©ç 
    pred_mask = np.argmax(output_tensor, axis=1).squeeze()
    
    # 2. è½¬æ¢ä¸ºå¯è§†åŒ–æ ¼å¼
    vis_mask = (pred_mask * 255).astype(np.uint8)
    
    # 3. ï¼ˆå¯é€‰ï¼‰ç¡®ä¿å°ºå¯¸å®Œå…¨ä¸€è‡´ï¼Œå¯¹äºå°ºå¯¸åŒ¹é…çš„æƒ…å†µï¼Œè¿™ä¸€æ­¥å‡ ä¹æ— å¼€é”€
    return cv2.resize(vis_mask, (original_width, original_height), interpolation=cv2.INTER_NEAREST)


# --- æ‘„åƒå¤´æŠ“å–çº¿ç¨‹ (æ— ä¿®æ”¹) ---
def camera_capture_thread():
    print("æ­£åœ¨æ‰“å¼€æ‘„åƒå¤´...")
    cap = cv2.VideoCapture(CAMERA_INDEX, cv2.CAP_V4L2)
    if not cap.isOpened():
        print(f"é”™è¯¯: æ— æ³•æ‰“å¼€æ‘„åƒå¤´ç´¢å¼• {CAMERA_INDEX}.")
        return

    # è®¾ç½®æ‘„åƒå¤´å‚æ•°ï¼Œç¡®ä¿è¾“å‡ºæ˜¯æˆ‘ä»¬æœŸæœ›çš„å°ºå¯¸
    fourcc = cv2.VideoWriter_fourcc(*'MJPG')
    cap.set(cv2.CAP_PROP_FOURCC, fourcc)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, CAMERA_WIDTH)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, CAMERA_HEIGHT)
    cap.set(cv2.CAP_PROP_FPS, 30)
    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)

    # å†æ¬¡ç¡®è®¤æ‘„åƒå¤´çš„å®é™…è¾“å‡ºå‚æ•°
    actual_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    actual_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    actual_fps = cap.get(cv2.CAP_PROP_FPS)
    print("\n--- æ‘„åƒå¤´å®é™…å‚æ•° ---")
    print(f"åˆ†è¾¨ç‡: {actual_w}x{actual_h}, å¸§ç‡: {actual_fps}")
    print("---------------------------\n")

    if actual_w != CAMERA_WIDTH or actual_h != CAMERA_HEIGHT:
        print(f"âš ï¸ è­¦å‘Š: æ‘„åƒå¤´å®é™…è¾“å‡º ({actual_w}x{actual_h}) ä¸æœŸæœ› ({CAMERA_WIDTH}x{CAMERA_HEIGHT}) ä¸ç¬¦!")

    while True:
        ret, frame = cap.read()
        if not ret:
            time.sleep(0.1)
            continue
        try:
            if frame_queue.empty():
                frame_queue.put_nowait(frame)
        except queue.Full:
            pass
    cap.release()

# --- æ¨ç†çº¿ç¨‹ (ä½¿ç”¨æç®€åŒ–çš„é¢„å¤„ç†å’Œåå¤„ç†) ---
def inference_thread():
    global stats_data, data_lock
    print(f"æ­£åœ¨åŠ è½½[åˆ†è¾¨ç‡åŒ¹é…]çš„æ¨¡å‹: {MODEL_PATH}")
    model = InferSession(DEVICE_ID, MODEL_PATH)
    print("æ¨¡å‹åŠ è½½å®Œæˆã€‚")
    
    frame_count = 0
    total_times = {"preprocess": 0, "inference": 0, "postprocess": 0, "pipeline": 0}

    print("\n=== ğŸš€ åˆ†è¾¨ç‡åŒ¹é…-æè‡´æ€§èƒ½ç›‘æ§ ğŸš€ ===")
    print("ğŸ’¡ é¢„å¤„ç†: æ— Resize/Paddingï¼Œä»…æ ¼å¼è½¬æ¢ï¼ŒCPUå¼€é”€æœ€å°åŒ–ï¼")
    print("æ¯20å¸§è¾“å‡ºä¸€æ¬¡è¯¦ç»†æ€§èƒ½åˆ†æ...")

    while True:
        try:
            frame = frame_queue.get(timeout=1)
        except queue.Empty:
            continue

        loop_start_time = time.time()
        cam_height, cam_width = frame.shape[:2]
        
        # --- æç®€CPUé¢„å¤„ç† ---
        preprocess_start = time.time()
        input_data = preprocess_matched_resolution(frame, dtype=np.float16)
        preprocess_time_ms = (time.time() - preprocess_start) * 1000
        
        # --- NPUæ¨ç† ---
        infer_start_time = time.time()
        outputs = model.infer([input_data])
        inference_time_ms = (time.time() - infer_start_time) * 1000
        
        # --- æç®€CPUåå¤„ç† ---
        postprocess_start = time.time()
        lane_mask = postprocess_matched_resolution(outputs[0], cam_width, cam_height)
        postprocess_time_ms = (time.time() - postprocess_start) * 1000
        
        pipeline_latency_ms = (time.time() - loop_start_time) * 1000
        
        # ç»Ÿè®¡å’Œæ‰“å°
        frame_count += 1
        total_times["preprocess"] += preprocess_time_ms
        total_times["inference"] += inference_time_ms
        total_times["postprocess"] += postprocess_time_ms
        total_times["pipeline"] += pipeline_latency_ms
        
        if frame_count % 20 == 0:
            avg_preprocess = total_times["preprocess"] / frame_count
            avg_inference = total_times["inference"] / frame_count
            avg_postprocess = total_times["postprocess"] / frame_count
            avg_pipeline = total_times["pipeline"] / frame_count
            
            print(f"\n--- âš¡ ç¬¬{frame_count}å¸§æ€§èƒ½åˆ†æ (åˆ†è¾¨ç‡åŒ¹é…) ---")
            print(f"è¾“å…¥ -> æ¨¡å‹: {cam_width}x{cam_height} -> {MODEL_WIDTH}x{MODEL_HEIGHT} (å®Œç¾åŒ¹é…)")
            print(f"ğŸ¯ æ•°æ®ç±»å‹: {str(input_data.dtype).upper()}")
            print(f"ã€CPUé¢„å¤„ç†ã€‘: {preprocess_time_ms:.1f}ms (å¹³å‡: {avg_preprocess:.1f}ms) âš¡")
            print(f"ã€NPU æ¨ç†ã€‘: {inference_time_ms:.1f}ms (å¹³å‡: {avg_inference:.1f}ms) ğŸš€")
            print(f"ã€CPUåå¤„ç†ã€‘: {postprocess_time_ms:.1f}ms (å¹³å‡: {avg_postprocess:.1f}ms)")
            print(f"--------------------------------------------------")
            print(f"ã€æµæ°´çº¿æ€»å»¶è¿Ÿã€‘: {pipeline_latency_ms:.1f}ms (ç†è®ºFPS: {1000/pipeline_latency_ms:.1f})")
            print(f"ã€å¹³å‡æ€»å»¶è¿Ÿã€‘  : {avg_pipeline:.1f}ms (å¹³å‡FPS: {1000/avg_pipeline:.1f})")
            print("=" * 60)

        # ç»“æœé˜Ÿåˆ—å’Œæ•°æ®é”
        try:
            if result_queue.empty():
                result_queue.put_nowait({
                    "frame": frame, "mask": lane_mask,
                    "latency": pipeline_latency_ms, "inference_time": inference_time_ms,
                    "preprocess_time": preprocess_time_ms, "postprocess_time": postprocess_time_ms
                })
        except queue.Full:
            pass
        
        with data_lock:
            stats_data["pipeline_latency"] = f"{pipeline_latency_ms:.1f}"
            stats_data["inference_time"] = f"{inference_time_ms:.1f}"
            stats_data["preprocess_time"] = f"{preprocess_time_ms:.1f}"
            stats_data["postprocess_time"] = f"{postprocess_time_ms:.1f}"


# --- NPU/CPU ç›‘æ§çº¿ç¨‹, Flaskè·¯ç”±å’Œä¸»å‡½æ•° (ä¿æŒä¸å˜) ---
def system_monitor_loop():
    global stats_data, data_lock
    npu_error_printed = False
    
    while True:
        cpu = psutil.cpu_percent()
        mem = psutil.virtual_memory().percent
        with data_lock:
            stats_data["cpu_percent"] = f"{cpu:.1f}"
            stats_data["mem_percent"] = f"{mem:.1f}"

        try:
            result = subprocess.run([NPU_SMI_PATH, 'info'], capture_output=True, text=True, check=True, timeout=2)
            output = result.stdout
            util_match = re.search(r'NPU\s+Utilization\s*:\s*(\d+)\s*%', output)
            mem_match = re.search(r'Memory\s+Usage\s*:\s*([\d\.]+)\s*MiB\s*/\s*([\d\.]+)\s*MiB', output)
            npu_util = util_match.group(1) if util_match else "N/A"
            npu_mem = f"{float(mem_match.group(1)):.0f} / {float(mem_match.group(2)):.0f} MiB" if mem_match else "N/A"
            with data_lock:
                stats_data["npu_util"] = npu_util
                stats_data["npu_mem"] = npu_mem
            if npu_error_printed:
                print("âœ… NPUç›‘æ§å·²æ¢å¤æ­£å¸¸")
                npu_error_printed = False
        except Exception:
             if not npu_error_printed:
                 print(f"âŒ NPUç›‘æ§å¤±è´¥ï¼Œè¯·æ£€æŸ¥è·¯å¾„æˆ–æƒé™: {NPU_SMI_PATH}")
                 npu_error_printed = True
             with data_lock:
                 stats_data["npu_util"] = "Error"
                 stats_data["npu_mem"] = "Error"
        time.sleep(1)

HTML_TEMPLATE = """
<!DOCTYPE html>
<html>
<head>
    <title>è½¦é“çº¿æ£€æµ‹ (åˆ†è¾¨ç‡åŒ¹é…ä¼˜åŒ–ç‰ˆ)</title>
    <!-- ... æ­¤å¤„çœç•¥CSSæ ·å¼ï¼Œä¸ä¹‹å‰ç›¸åŒ ... -->
    <style>body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif; margin: 0; padding: 20px; background-color: #f4f7f6; color: #333; } .container { max-width: 1400px; margin: 0 auto; background-color: white; padding: 20px; border-radius: 10px; box-shadow: 0 4px 15px rgba(0,0,0,0.05); } h1 { text-align: center; color: #1a73e8; } .main-content { display: flex; flex-wrap: wrap; gap: 20px; } .video-container { flex: 3; min-width: 600px; } #videoStream { width: 100%; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); background-color: #eee; } .stats-container { flex: 1; min-width: 300px; background-color: #f8f9fa; padding: 20px; border-radius: 8px; } .stats-container h2 { margin-top: 0; border-bottom: 2px solid #e0e0e0; padding-bottom: 10px; color: #3c4043; } .stat-grid { display: grid; grid-template-columns: 1fr; gap: 15px; } .stat-card { background-color: #fff; padding: 15px; border-radius: 5px; border-left: 5px solid #1a73e8; box-shadow: 0 1px 3px rgba(0,0,0,0.08); display: flex; justify-content: space-between; align-items: center; } .stat-card.npu { border-left-color: #34a853; } .stat-card.cpu { border-left-color: #fbbc05; } .stat-card.e2e { border-left-color: #ea4335; } .stat-label { font-size: 14px; color: #5f6368; } .stat-value { font-size: 18px; font-weight: 600; color: #202124; } .optimization-badge { background: linear-gradient(45deg, #34A853, #4285F4); color: white; padding: 5px 10px; border-radius: 15px; font-size: 12px; font-weight: bold; margin-left: 10px; }</style>
</head>
<body>
    <div class="container">
        <h1>ğŸš€ è½¦é“çº¿æ£€æµ‹ <span class="optimization-badge">åˆ†è¾¨ç‡åŒ¹é…-æ€§èƒ½æœ€ä¼˜</span></h1>
        <!-- ... æ­¤å¤„çœç•¥HTMLç»“æ„ï¼Œä¸ä¹‹å‰ç›¸åŒ ... -->
        <div class="main-content"><div class="video-container"><img id="videoStream" src="{{ url_for('video_feed') }}" alt="Live Stream"></div><div class="stats-container"><h2>ğŸ“Š æ€§èƒ½ç›‘æ§</h2><div class="stat-grid"><div class="stat-card"><span class="stat-label">æ˜¾ç¤ºå¸§ç‡ (FPS)</span><span id="fps" class="stat-value">--</span></div><div class="stat-card e2e"><span class="stat-label">ç«¯åˆ°ç«¯æµæ°´çº¿å»¶è¿Ÿ (ms)</span><span id="pipeline_latency" class="stat-value">--</span></div><div class="stat-card e2e"><span class="stat-label">NPU æ¨ç† (ms)</span><span id="inference_time" class="stat-value">--</span></div><div class="stat-card"><span class="stat-label">CPUé¢„å¤„ç† (ms)</span><span id="preprocess_time" class="stat-value">--</span></div><div class="stat-card"><span class="stat-label">åå¤„ç†è€—æ—¶ (ms)</span><span id="postprocess_time" class="stat-value">--</span></div><div class="stat-card npu"><span class="stat-label">NPU åˆ©ç”¨ç‡ (%)</span><span id="npu_util" class="stat-value">--</span></div><div class="stat-card npu"><span class="stat-label">NPU å†…å­˜å ç”¨</span><span id="npu_mem" class="stat-value">--</span></div><div class="stat-card cpu"><span class="stat-label">CPU åˆ©ç”¨ç‡ (%)</span><span id="cpu_percent" class="stat-value">--</span></div><div class="stat-card cpu"><span class="stat-label">ç³»ç»Ÿå†…å­˜å ç”¨ (%)</span><span id="mem_percent" class="stat-value">--</span></div></div></div></div>
    </div>
    <!-- ... æ­¤å¤„çœç•¥JSè„šæœ¬ï¼Œä¸ä¹‹å‰ç›¸åŒ ... -->
    <script>function refreshStats() { fetch('/stats').then(response => response.json()).then(data => { document.getElementById('fps').textContent = data.fps; document.getElementById('pipeline_latency').textContent = data.pipeline_latency + ' ms'; document.getElementById('inference_time').textContent = data.inference_time + ' ms'; document.getElementById('preprocess_time').textContent = data.preprocess_time + ' ms'; document.getElementById('postprocess_time').textContent = data.postprocess_time + ' ms'; document.getElementById('npu_util').textContent = data.npu_util; document.getElementById('npu_mem').textContent = data.npu_mem; document.getElementById('cpu_percent').textContent = data.cpu_percent + ' %'; document.getElementById('mem_percent').textContent = data.mem_percent + ' %'; }).catch(error => console.error('è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥:', error)); } window.onload = function() { refreshStats(); setInterval(refreshStats, 1000); };</script>
</body>
</html>
"""

@app.route("/")
def index():
    return render_template_string(HTML_TEMPLATE)

@app.route("/video_feed")
def video_feed():
    def generate():
        global stats_data, data_lock
        frame_count = 0
        start_time = time.time()
        while True:
            try:
                result = result_queue.get(timeout=1)
            except queue.Empty:
                continue
            frame = result["frame"]
            mask = result["mask"]
            green_overlay = np.zeros_like(frame, dtype=np.uint8)
            green_overlay[mask > 0] = [0, 255, 0]
            vis_frame = cv2.addWeighted(frame, 1.0, green_overlay, 0.5, 0)
            frame_count += 1
            elapsed = time.time() - start_time
            if elapsed > 1.0:
                fps = frame_count / elapsed
                with data_lock:
                    stats_data["fps"] = f"{fps:.1f}"
                frame_count = 0
                start_time = time.time()
            with data_lock:
                fps_text = f"Display FPS: {stats_data['fps']}"
                latency_text = f"Latency: {stats_data['pipeline_latency']} ms"
            cv2.putText(vis_frame, fps_text, (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
            cv2.putText(vis_frame, latency_text, (15, 65), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
            (flag, encodedImage) = cv2.imencode(".jpg", vis_frame)
            if not flag:
                continue
            yield(b'--frame\r\n' b'Content-Type: image/jpeg\r\n\r\n' + 
                  bytearray(encodedImage) + b'\r\n')
    return Response(generate(), mimetype="multipart/x-mixed-replace; boundary=frame")

@app.route("/stats")
def stats():
    with data_lock:
        return jsonify(stats_data)

if __name__ == '__main__':
    print("ğŸš€ è½¦é“çº¿æ£€æµ‹ [åˆ†è¾¨ç‡åŒ¹é…] å®æ—¶æ¨ç†ç³»ç»Ÿå¯åŠ¨")
    print("=============================================================")
    print(f"ğŸ§  æ¨¡å‹: {MODEL_PATH}")
    print(f"ğŸ¯ è¾“å…¥å°ºå¯¸: {MODEL_WIDTH}x{MODEL_HEIGHT} (ä¸æ‘„åƒå¤´åŒ¹é…)")
    print(f"âš¡ ä¼˜åŒ–: æ— éœ€Resize/Paddingï¼ŒCPUé¢„å¤„ç†å¼€é”€å·²é™è‡³æœ€ä½ï¼")
    print("=============================================================")
    
    Thread(target=camera_capture_thread, daemon=True).start()
    Thread(target=inference_thread, daemon=True).start()
    Thread(target=system_monitor_loop, daemon=True).start()
    
    print("\nWebæœåŠ¡å™¨å·²å¯åŠ¨ã€‚è¯·åœ¨æµè§ˆå™¨ä¸­è®¿é—®: http://<Your_Atlas_IP>:8000")
    print("ç»ˆç«¯å°†æ˜¾ç¤ºæ€§èƒ½åˆ†æ...")
    app.run(host='0.0.0.0', port=8000, threaded=True)