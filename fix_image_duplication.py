#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å›¾ç‰‡å»é‡å’Œä¼˜åŒ–è„šæœ¬
ç”¨äºæ£€æŸ¥å¹¶ä¿®å¤æ¨ç†è„šæœ¬ä¸­å¯èƒ½çš„é‡å¤å¤„ç†é—®é¢˜
ä¼˜åŒ–æ•°æ®åŠ è½½å’Œæ¨ç†æµç¨‹
"""

import os
import sys
import hashlib
import shutil
from pathlib import Path
from collections import defaultdict
import cv2
import numpy as np
from PIL import Image

def calculate_image_hash(image_path):
    """è®¡ç®—å›¾ç‰‡çš„MD5å“ˆå¸Œå€¼"""
    try:
        with open(image_path, 'rb') as f:
            image_hash = hashlib.md5(f.read()).hexdigest()
        return image_hash
    except Exception as e:
        print(f"Error calculating hash for {image_path}: {e}")
        return None

def calculate_visual_hash(image_path, hash_size=8):
    """è®¡ç®—å›¾ç‰‡çš„æ„ŸçŸ¥å“ˆå¸Œå€¼ï¼ˆç”¨äºæ£€æµ‹ç›¸ä¼¼å›¾ç‰‡ï¼‰"""
    try:
        # è¯»å–å›¾ç‰‡å¹¶è½¬æ¢ä¸ºç°åº¦
        img = cv2.imread(str(image_path))
        if img is None:
            return None
        
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        
        # ç¼©æ”¾åˆ°å›ºå®šå¤§å°
        resized = cv2.resize(gray, (hash_size, hash_size))
        
        # è®¡ç®—å¹³å‡å€¼
        avg = resized.mean()
        
        # ç”Ÿæˆå“ˆå¸Œ
        hash_bits = []
        for i in range(hash_size):
            for j in range(hash_size):
                hash_bits.append('1' if resized[i, j] > avg else '0')
        
        return ''.join(hash_bits)
    except Exception as e:
        print(f"Error calculating visual hash for {image_path}: {e}")
        return None

def hamming_distance(hash1, hash2):
    """è®¡ç®—ä¸¤ä¸ªå“ˆå¸Œå€¼ä¹‹é—´çš„æ±‰æ˜è·ç¦»"""
    if len(hash1) != len(hash2):
        return float('inf')
    return sum(c1 != c2 for c1, c2 in zip(hash1, hash2))

def find_duplicate_images(directory, similarity_threshold=5):
    """æŸ¥æ‰¾é‡å¤å’Œç›¸ä¼¼çš„å›¾ç‰‡"""
    print(f"æ‰«æç›®å½•: {directory}")
    
    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.gif'}
    image_files = []
    
    # æ”¶é›†æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
    for root, dirs, files in os.walk(directory):
        for file in files:
            if Path(file).suffix.lower() in image_extensions:
                image_files.append(Path(root) / file)
    
    print(f"æ‰¾åˆ° {len(image_files)} ä¸ªå›¾ç‰‡æ–‡ä»¶")
    
    # è®¡ç®—å“ˆå¸Œå€¼
    exact_hashes = {}  # MD5å“ˆå¸Œ -> [æ–‡ä»¶è·¯å¾„åˆ—è¡¨]
    visual_hashes = {}  # æ„ŸçŸ¥å“ˆå¸Œ -> [æ–‡ä»¶è·¯å¾„åˆ—è¡¨]
    
    for i, image_path in enumerate(image_files):
        print(f"å¤„ç† [{i+1}/{len(image_files)}]: {image_path.name}")
        
        # è®¡ç®—ç²¾ç¡®å“ˆå¸Œï¼ˆMD5ï¼‰
        exact_hash = calculate_image_hash(image_path)
        if exact_hash:
            if exact_hash not in exact_hashes:
                exact_hashes[exact_hash] = []
            exact_hashes[exact_hash].append(image_path)
        
        # è®¡ç®—æ„ŸçŸ¥å“ˆå¸Œ
        visual_hash = calculate_visual_hash(image_path)
        if visual_hash:
            if visual_hash not in visual_hashes:
                visual_hashes[visual_hash] = []
            visual_hashes[visual_hash].append(image_path)
    
    # æŸ¥æ‰¾ç²¾ç¡®é‡å¤
    exact_duplicates = {h: paths for h, paths in exact_hashes.items() if len(paths) > 1}
    
    # æŸ¥æ‰¾ç›¸ä¼¼å›¾ç‰‡
    similar_groups = []
    processed_hashes = set()
    
    for hash1, paths1 in visual_hashes.items():
        if hash1 in processed_hashes:
            continue
        
        similar_group = list(paths1)
        processed_hashes.add(hash1)
        
        for hash2, paths2 in visual_hashes.items():
            if hash2 in processed_hashes:
                continue
            
            if hamming_distance(hash1, hash2) <= similarity_threshold:
                similar_group.extend(paths2)
                processed_hashes.add(hash2)
        
        if len(similar_group) > 1:
            similar_groups.append(similar_group)
    
    return exact_duplicates, similar_groups

def remove_duplicates(exact_duplicates, backup_dir=None):
    """ç§»é™¤é‡å¤å›¾ç‰‡ï¼Œä¿ç•™ç¬¬ä¸€ä¸ª"""
    removed_count = 0
    
    if backup_dir:
        os.makedirs(backup_dir, exist_ok=True)
    
    for hash_value, paths in exact_duplicates.items():
        print(f"\nå‘ç°é‡å¤ç»„ (hash: {hash_value[:8]}...):")
        for i, path in enumerate(paths):
            print(f"  {i+1}. {path}")
        
        # ä¿ç•™ç¬¬ä¸€ä¸ªæ–‡ä»¶ï¼Œç§»é™¤å…¶ä»–
        keep_file = paths[0]
        remove_files = paths[1:]
        
        print(f"ä¿ç•™: {keep_file}")
        
        for remove_file in remove_files:
            try:
                if backup_dir:
                    # å¤‡ä»½åˆ°æŒ‡å®šç›®å½•
                    backup_path = Path(backup_dir) / f"dup_{remove_file.name}"
                    shutil.move(str(remove_file), str(backup_path))
                    print(f"ç§»åŠ¨åˆ°å¤‡ä»½: {remove_file} -> {backup_path}")
                else:
                    # ç›´æ¥åˆ é™¤
                    os.remove(remove_file)
                    print(f"åˆ é™¤: {remove_file}")
                removed_count += 1
            except Exception as e:
                print(f"åˆ é™¤å¤±è´¥ {remove_file}: {e}")
    
    return removed_count

def optimize_directory_structure(base_dir):
    """ä¼˜åŒ–ç›®å½•ç»“æ„ï¼Œç¡®ä¿æ²¡æœ‰é‡å¤æ–‡ä»¶"""
    print(f"\nä¼˜åŒ–ç›®å½•ç»“æ„: {base_dir}")
    
    directories_to_check = [
        'data/custom/images',
        'bdd100k/images',
        'static/uploads'
    ]
    
    for rel_dir in directories_to_check:
        full_dir = Path(base_dir) / rel_dir
        if full_dir.exists():
            print(f"\næ£€æŸ¥ç›®å½•: {full_dir}")
            exact_duplicates, similar_groups = find_duplicate_images(full_dir)
            
            if exact_duplicates:
                print(f"å‘ç° {len(exact_duplicates)} ç»„ç²¾ç¡®é‡å¤")
                backup_dir = full_dir / 'duplicates_backup'
                removed = remove_duplicates(exact_duplicates, backup_dir)
                print(f"ç§»é™¤ {removed} ä¸ªé‡å¤æ–‡ä»¶")
            else:
                print("æœªå‘ç°ç²¾ç¡®é‡å¤")
            
            if similar_groups:
                print(f"å‘ç° {len(similar_groups)} ç»„ç›¸ä¼¼å›¾ç‰‡:")
                for i, group in enumerate(similar_groups):
                    print(f"  ç›¸ä¼¼ç»„ {i+1}: {len(group)} ä¸ªæ–‡ä»¶")
                    for path in group:
                        print(f"    - {path.name}")
            else:
                print("æœªå‘ç°ç›¸ä¼¼å›¾ç‰‡")

def check_inference_scripts():
    """æ£€æŸ¥æ¨ç†è„šæœ¬ä¸­çš„æ½œåœ¨é‡å¤å¤„ç†é—®é¢˜"""
    print("\næ£€æŸ¥æ¨ç†è„šæœ¬...")
    
    scripts_to_check = [
        'test_custom_images.py',
        'test_bdd100k_inference.py', 
        'inference_test.py',
        'test_onnx_inference.py'
    ]
    
    issues_found = []
    
    for script in scripts_to_check:
        if os.path.exists(script):
            print(f"æ£€æŸ¥è„šæœ¬: {script}")
            
            with open(script, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æ£€æŸ¥å¯èƒ½çš„é‡å¤å¤„ç†æ¨¡å¼
            issues = []
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤çš„æ–‡ä»¶æ‰©å±•åå¤„ç†
            if content.count('image_extensions') > 1:
                issues.append("å¯èƒ½å­˜åœ¨é‡å¤çš„æ–‡ä»¶æ‰©å±•åå®šä¹‰")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤çš„globæ¨¡å¼
            if content.count('.glob(') > 2:
                issues.append("å¯èƒ½å­˜åœ¨é‡å¤çš„æ–‡ä»¶æœç´¢")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤çš„å›¾ç‰‡åŠ è½½
            if content.count('cv2.imread') > 1 and content.count('for') > 1:
                issues.append("å¯èƒ½åœ¨å¾ªç¯ä¸­é‡å¤åŠ è½½å›¾ç‰‡")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤çš„é¢„å¤„ç†
            if content.count('preprocess') > 2:
                issues.append("å¯èƒ½å­˜åœ¨é‡å¤çš„é¢„å¤„ç†è°ƒç”¨")
            
            if issues:
                issues_found.append((script, issues))
                print(f"  âš ï¸  å‘ç°æ½œåœ¨é—®é¢˜:")
                for issue in issues:
                    print(f"    - {issue}")
            else:
                print(f"  âœ…  æœªå‘ç°æ˜æ˜¾é—®é¢˜")
    
    return issues_found

def create_optimized_inference_script():
    """åˆ›å»ºä¼˜åŒ–çš„æ¨ç†è„šæœ¬æ¨¡æ¿"""
    script_content = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–çš„å›¾ç‰‡æ¨ç†è„šæœ¬ - æ— é‡å¤å¤„ç†ç‰ˆæœ¬
é¿å…é‡å¤åŠ è½½ã€é‡å¤é¢„å¤„ç†ç­‰é—®é¢˜
"""

import os
import time
import cv2
import numpy as np
import torch
from pathlib import Path
from collections import OrderedDict

class OptimizedInferenceEngine:
    """ä¼˜åŒ–çš„æ¨ç†å¼•æ“ï¼Œé¿å…é‡å¤å¤„ç†"""
    
    def __init__(self, model, device='cuda'):
        self.model = model
        self.device = device
        self.processed_files = set()  # è®°å½•å·²å¤„ç†çš„æ–‡ä»¶ï¼Œé¿å…é‡å¤
        self.image_cache = OrderedDict()  # å›¾ç‰‡ç¼“å­˜
        self.max_cache_size = 10  # æœ€å¤§ç¼“å­˜å›¾ç‰‡æ•°é‡
        
    def get_image_files(self, directory, extensions=None):
        """è·å–å›¾ç‰‡æ–‡ä»¶åˆ—è¡¨ï¼Œå»é‡å¤„ç†"""
        if extensions is None:
            extensions = {'.jpg', '.jpeg', '.png', '.bmp'}
        
        # ä½¿ç”¨seté¿å…é‡å¤
        seen_files = set()
        image_files = []
        
        directory = Path(directory)
        for ext in extensions:
            # æœç´¢æ–‡ä»¶ï¼Œé¿å…é‡å¤æ·»åŠ 
            for pattern in [f'*{ext}', f'*{ext.upper()}']:
                for file_path in directory.glob(pattern):
                    # ä½¿ç”¨ç»å¯¹è·¯å¾„çš„å­—ç¬¦ä¸²ä½œä¸ºå”¯ä¸€æ ‡è¯†
                    file_key = str(file_path.resolve())
                    if file_key not in seen_files:
                        seen_files.add(file_key)
                        image_files.append(file_path)
        
        return sorted(image_files)
    
    def load_image_cached(self, image_path):
        """ç¼“å­˜å¼å›¾ç‰‡åŠ è½½ï¼Œé¿å…é‡å¤è¯»å–"""
        image_path = str(image_path)
        
        # æ£€æŸ¥ç¼“å­˜
        if image_path in self.image_cache:
            # ç§»åŠ¨åˆ°æœ€åï¼ˆLRUï¼‰
            self.image_cache.move_to_end(image_path)
            return self.image_cache[image_path]
        
        # è¯»å–å›¾ç‰‡
        image = cv2.imread(image_path)
        if image is None:
            raise ValueError(f"Cannot load image: {image_path}")
        
        # æ·»åŠ åˆ°ç¼“å­˜
        self.image_cache[image_path] = image
        
        # é™åˆ¶ç¼“å­˜å¤§å°
        if len(self.image_cache) > self.max_cache_size:
            self.image_cache.popitem(last=False)
        
        return image
    
    def preprocess_once(self, image, target_size=(1024, 2048)):
        """å•æ¬¡é¢„å¤„ç†ï¼Œé¿å…é‡å¤"""
        # åªåœ¨éœ€è¦æ—¶è¿›è¡Œresize
        if image.shape[:2] != target_size:
            image = cv2.resize(image, target_size)
        
        # è½¬æ¢ä¸ºtensorï¼ˆä¸€æ¬¡æ€§å®Œæˆæ‰€æœ‰è½¬æ¢ï¼‰
        image_tensor = torch.from_numpy(image.transpose(2, 0, 1)).float() / 255.0
        image_tensor = image_tensor.unsqueeze(0).to(self.device)
        
        return image_tensor
    
    def inference_batch(self, image_paths, batch_size=4):
        """æ‰¹é‡æ¨ç†ï¼Œæé«˜æ•ˆç‡"""
        results = []
        
        for i in range(0, len(image_paths), batch_size):
            batch_paths = image_paths[i:i+batch_size]
            
            # æ‰¹é‡åŠ è½½å’Œé¢„å¤„ç†
            batch_tensors = []
            batch_originals = []
            
            for path in batch_paths:
                if str(path) in self.processed_files:
                    print(f"è·³è¿‡å·²å¤„ç†æ–‡ä»¶: {path}")
                    continue
                
                try:
                    image = self.load_image_cached(path)
                    tensor = self.preprocess_once(image)
                    batch_tensors.append(tensor)
                    batch_originals.append(image)
                    self.processed_files.add(str(path))
                except Exception as e:
                    print(f"å¤„ç†å¤±è´¥ {path}: {e}")
                    continue
            
            if not batch_tensors:
                continue
            
            # æ‰¹é‡æ¨ç†
            try:
                batch_tensor = torch.cat(batch_tensors, dim=0)
                with torch.no_grad():
                    predictions = self.model(batch_tensor)
                
                # å¤„ç†ç»“æœ
                for j, (path, pred, orig) in enumerate(zip(batch_paths, predictions, batch_originals)):
                    results.append({
                        'path': path,
                        'prediction': pred.cpu().numpy(),
                        'original': orig
                    })
                    
            except Exception as e:
                print(f"æ‰¹é‡æ¨ç†å¤±è´¥: {e}")
                continue
        
        return results

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºä¼˜åŒ–çš„æ¨ç†æµç¨‹"""
    # è¿™é‡Œåº”è¯¥åŠ è½½å®é™…çš„æ¨¡å‹
    # model = load_your_model()
    # engine = OptimizedInferenceEngine(model)
    
    # è·å–å›¾ç‰‡æ–‡ä»¶ï¼ˆå»é‡ï¼‰
    # image_files = engine.get_image_files('data/custom/images')
    # print(f"æ‰¾åˆ° {len(image_files)} ä¸ªå”¯ä¸€å›¾ç‰‡æ–‡ä»¶")
    
    # æ‰¹é‡æ¨ç†
    # results = engine.inference_batch(image_files)
    # print(f"æˆåŠŸå¤„ç† {len(results)} ä¸ªæ–‡ä»¶")
    
    print("ä¼˜åŒ–æ¨ç†è„šæœ¬æ¨¡æ¿å·²ç”Ÿæˆ")

if __name__ == '__main__':
    main()
'''
    
    with open('optimized_inference_template.py', 'w', encoding='utf-8') as f:
        f.write(script_content)
    
    print("å·²åˆ›å»ºä¼˜åŒ–çš„æ¨ç†è„šæœ¬æ¨¡æ¿: optimized_inference_template.py")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” å›¾ç‰‡å»é‡å’Œæ¨ç†ä¼˜åŒ–å·¥å…·")
    print("=" * 50)
    
    # è·å–å½“å‰å·¥ä½œç›®å½•
    base_dir = os.getcwd()
    print(f"å·¥ä½œç›®å½•: {base_dir}")
    
    # 1. æ£€æŸ¥æ¨ç†è„šæœ¬
    print("\nğŸ“ æ£€æŸ¥æ¨ç†è„šæœ¬...")
    issues = check_inference_scripts()
    
    if issues:
        print(f"\nâš ï¸  å‘ç° {len(issues)} ä¸ªè„šæœ¬å­˜åœ¨æ½œåœ¨é—®é¢˜:")
        for script, script_issues in issues:
            print(f"\n{script}:")
            for issue in script_issues:
                print(f"  - {issue}")
    else:
        print("\nâœ… æ‰€æœ‰æ¨ç†è„šæœ¬æ£€æŸ¥æ­£å¸¸")
    
    # 2. ä¼˜åŒ–ç›®å½•ç»“æ„
    print("\nğŸ“ ä¼˜åŒ–ç›®å½•ç»“æ„...")
    optimize_directory_structure(base_dir)
    
    # 3. åˆ›å»ºä¼˜åŒ–æ¨¡æ¿
    print("\nğŸ› ï¸  åˆ›å»ºä¼˜åŒ–æ¨¡æ¿...")
    create_optimized_inference_script()
    
    print("\nâœ… ä¼˜åŒ–å®Œæˆ!")
    print("\nå»ºè®®:")
    print("1. æŸ¥çœ‹ç”Ÿæˆçš„ optimized_inference_template.py æ¨¡æ¿")
    print("2. æ£€æŸ¥å¤‡ä»½ç›®å½•ä¸­çš„é‡å¤æ–‡ä»¶")
    print("3. æ ¹æ®å‘ç°çš„é—®é¢˜ä¿®æ”¹ç›¸å…³æ¨ç†è„šæœ¬")

if __name__ == '__main__':
    main()
